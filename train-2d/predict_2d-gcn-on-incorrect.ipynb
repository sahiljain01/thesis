{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from pebble import lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing PyG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_coefficient(G, node):\n",
    "    ns = [n for n in G.neighbors(node)]\n",
    "    if len(ns) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = len(ns) * (len(ns) - 1) / 2\n",
    "    for i in range(0, len(ns)):\n",
    "        for j in range(i+1, len(ns)):\n",
    "            n1, n2 = ns[i], ns[j]\n",
    "            numerator += G.has_edge(n1, n2)\n",
    "    \n",
    "    return numerator / denominator\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(G):\n",
    "    x = torch.randn(G.number_of_nodes(), 4)\n",
    "    ind = 0\n",
    "    for node in G.nodes():\n",
    "        x[ind][0] = 1 # uniform\n",
    "        x[ind][1] = G.degree[node] # node degree as a scalar \n",
    "        x[ind][2] = clustering_coefficient(G, node) # triangle counting?\n",
    "        x[ind][3] = ind # node ID features\n",
    "        ind += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        for ind, graph in enumerate(total_laman_data[0]):\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        for ind, graph in enumerate(total_laman_data[1]):\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/incorrectly-predicted.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_data = LamanDataset(\"\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 64], x=[15, 4], label=[1], num_nodes=15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laman_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "proportions = [.7, .3]\n",
    "lengths = [int(p * len(laman_data)) for p in proportions]\n",
    "lengths[-1] = len(laman_data) - sum(lengths[:-1])\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_data, test_data = random_split(laman_data, lengths, generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = 256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches:  27\n",
      "Number of test batches:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches: \", len(train_loader))\n",
    "print(\"Number of test batches: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 14974], x=[3840, 4], label=[256], num_nodes=3840, batch=[3840], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_gcn.gcn import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (initial_conv): GCNConv(4, 15)\n",
      "  (conv1): GCNConv(15, 15)\n",
      "  (out): Linear(in_features=60, out_features=60, bias=True)\n",
      "  (out2): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  4036\n"
     ]
    }
   ],
   "source": [
    "model = GCN(num_features=4, embedding_size=15)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=0.001)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', min_lr=1e-6, verbose=True, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, features_to_use):\n",
    "    ind = 0\n",
    "    model.train()\n",
    "    for batch in data:\n",
    "        optimizer.zero_grad()\n",
    "        pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "        pred = torch.squeeze(pred)\n",
    "        loss = loss_fn(pred.float(), batch.label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ind += 1\n",
    "\n",
    "    return loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader, features_to_use):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "            pred = torch.squeeze(pred)\n",
    "            y = batch.label\n",
    "            predictions = (pred > 0.5).long() \n",
    "            num_correct += (predictions == y).sum() \n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "    return float(num_correct)/float(num_samples)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Train loss 0.6999048590660095\n",
      "Train Accuracy 55.693950177935946 | Test Accuracy 55.51712210307852\n",
      "Epoch 1 | Train loss 0.7237244248390198\n",
      "Train Accuracy 55.693950177935946 | Test Accuracy 55.51712210307852\n",
      "Epoch 2 | Train loss 0.6852387189865112\n",
      "Train Accuracy 57.07295373665481 | Test Accuracy 56.10515392597717\n",
      "Epoch 3 | Train loss 0.6905332207679749\n",
      "Train Accuracy 57.443653618030844 | Test Accuracy 56.10515392597717\n",
      "Epoch 4 | Train loss 0.689138650894165\n",
      "Train Accuracy 56.16844602609727 | Test Accuracy 55.932203389830505\n",
      "Epoch 5 | Train loss 0.6662771105766296\n",
      "Train Accuracy 56.791221826809014 | Test Accuracy 56.3472846765825\n",
      "Epoch 6 | Train loss 0.6785815358161926\n",
      "Train Accuracy 59.20818505338078 | Test Accuracy 56.416464891041166\n",
      "Epoch 7 | Train loss 0.6938939690589905\n",
      "Train Accuracy 60.5723606168446 | Test Accuracy 57.59252853683846\n",
      "Epoch 8 | Train loss 0.6713453531265259\n",
      "Train Accuracy 59.29715302491103 | Test Accuracy 57.938429609131795\n",
      "Epoch 9 | Train loss 0.6395963430404663\n",
      "Train Accuracy 57.680901542111506 | Test Accuracy 57.384987893462466\n",
      "Epoch 10 | Train loss 0.6677936315536499\n",
      "Train Accuracy 62.70759193357058 | Test Accuracy 60.25596679349706\n",
      "Epoch 11 | Train loss 0.6721253991127014\n",
      "Train Accuracy 64.96144721233689 | Test Accuracy 63.12694569353165\n",
      "Epoch 12 | Train loss 0.6780746579170227\n",
      "Train Accuracy 65.48042704626334 | Test Accuracy 63.12694569353165\n",
      "Epoch 13 | Train loss 0.6461741924285889\n",
      "Train Accuracy 67.83807829181495 | Test Accuracy 65.68661362850224\n",
      "Epoch 14 | Train loss 0.6137978434562683\n",
      "Train Accuracy 68.7129300118624 | Test Accuracy 67.10480802490487\n",
      "Epoch 15 | Train loss 0.6230812668800354\n",
      "Train Accuracy 59.99406880189798 | Test Accuracy 59.9792459356624\n",
      "Epoch 16 | Train loss 0.5900785326957703\n",
      "Train Accuracy 70.84816132858838 | Test Accuracy 70.2179176755448\n",
      "Epoch 17 | Train loss 0.5835950374603271\n",
      "Train Accuracy 70.7888493475682 | Test Accuracy 70.1141473538568\n",
      "Epoch 18 | Train loss 0.6077734231948853\n",
      "Train Accuracy 71.44128113879003 | Test Accuracy 70.84053960567277\n",
      "Epoch 19 | Train loss 0.573779821395874\n",
      "Train Accuracy 72.47924080664293 | Test Accuracy 71.87824282255275\n",
      "Epoch 20 | Train loss 0.5993174910545349\n",
      "Train Accuracy 72.49406880189798 | Test Accuracy 71.77447250086475\n",
      "Epoch 21 | Train loss 0.5594133734703064\n",
      "Train Accuracy 74.25860023724793 | Test Accuracy 73.6423382912487\n",
      "Epoch 22 | Train loss 0.5914292931556702\n",
      "Train Accuracy 74.11032028469751 | Test Accuracy 73.12348668280872\n",
      "Epoch 23 | Train loss 0.551917314529419\n",
      "Train Accuracy 75.29655990510084 | Test Accuracy 74.26496022137668\n",
      "Epoch 24 | Train loss 0.5737976431846619\n",
      "Train Accuracy 69.27639383155397 | Test Accuracy 68.45382220684884\n",
      "Epoch 25 | Train loss 0.5459323525428772\n",
      "Train Accuracy 75.93416370106762 | Test Accuracy 74.88758215150467\n",
      "Epoch 26 | Train loss 0.49267417192459106\n",
      "Train Accuracy 76.98695136417555 | Test Accuracy 75.9944655828433\n",
      "Epoch 27 | Train loss 0.6832025647163391\n",
      "Train Accuracy 75.69691577698696 | Test Accuracy 74.88758215150467\n",
      "Epoch 28 | Train loss 0.5282134413719177\n",
      "Train Accuracy 76.98695136417555 | Test Accuracy 76.40954686959529\n",
      "Epoch 29 | Train loss 0.46375229954719543\n",
      "Train Accuracy 77.80249110320284 | Test Accuracy 76.82462815634729\n",
      "Epoch 30 | Train loss 0.45003587007522583\n",
      "Train Accuracy 76.33451957295374 | Test Accuracy 75.025942580422\n",
      "Epoch 31 | Train loss 0.49489152431488037\n",
      "Train Accuracy 78.1731909845789 | Test Accuracy 77.4818401937046\n",
      "Epoch 32 | Train loss 0.48231402039527893\n",
      "Train Accuracy 78.03973902728352 | Test Accuracy 77.4818401937046\n",
      "Epoch 33 | Train loss 0.5200161933898926\n",
      "Train Accuracy 77.37247924080664 | Test Accuracy 76.44413697682462\n",
      "Epoch 34 | Train loss 0.49044859409332275\n",
      "Train Accuracy 78.63285883748517 | Test Accuracy 77.82774126599793\n",
      "Epoch 35 | Train loss 0.4605273902416229\n",
      "Train Accuracy 78.54389086595492 | Test Accuracy 77.8969214804566\n",
      "Epoch 36 | Train loss 0.45303380489349365\n",
      "Train Accuracy 78.82562277580071 | Test Accuracy 78.24282255274991\n",
      "Epoch 37 | Train loss 0.5286515951156616\n",
      "Train Accuracy 76.4976275207592 | Test Accuracy 75.16430300933933\n",
      "Epoch 38 | Train loss 0.5945371985435486\n",
      "Train Accuracy 78.88493475682088 | Test Accuracy 78.31200276720858\n",
      "Epoch 39 | Train loss 0.5650014281272888\n",
      "Train Accuracy 77.0759193357058 | Test Accuracy 76.09823590453131\n",
      "Epoch 40 | Train loss 0.5321038365364075\n",
      "Train Accuracy 79.06287069988137 | Test Accuracy 78.55413351781391\n",
      "Epoch 41 | Train loss 0.5502638220787048\n",
      "Train Accuracy 78.84045077105574 | Test Accuracy 78.17364233829124\n",
      "Epoch 42 | Train loss 0.45740821957588196\n",
      "Train Accuracy 75.02965599051008 | Test Accuracy 74.09200968523002\n",
      "Epoch 43 | Train loss 0.43559959530830383\n",
      "Train Accuracy 76.0379596678529 | Test Accuracy 75.025942580422\n",
      "Epoch 44 | Train loss 0.5080339312553406\n",
      "Train Accuracy 78.11387900355872 | Test Accuracy 77.75856105153926\n",
      "Epoch 45 | Train loss 0.5195035338401794\n",
      "Train Accuracy 76.9276393831554 | Test Accuracy 75.8906952611553\n",
      "Epoch 46 | Train loss 0.5504244565963745\n",
      "Train Accuracy 79.41874258600238 | Test Accuracy 79.1767554479419\n",
      "Epoch 47 | Train loss 0.5443839430809021\n",
      "Train Accuracy 79.43357058125741 | Test Accuracy 79.21134555517122\n",
      "Epoch 48 | Train loss 0.4817669987678528\n",
      "Train Accuracy 79.32977461447213 | Test Accuracy 78.62331373227256\n",
      "Epoch 49 | Train loss 0.48636385798454285\n",
      "Train Accuracy 78.20284697508897 | Test Accuracy 77.62020062262192\n",
      "Epoch 50 | Train loss 0.4148204028606415\n",
      "Train Accuracy 79.41874258600238 | Test Accuracy 78.69249394673123\n",
      "Epoch 51 | Train loss 0.41523221135139465\n",
      "Train Accuracy 79.75978647686833 | Test Accuracy 79.66101694915254\n",
      "Epoch 52 | Train loss 0.43067705631256104\n",
      "Train Accuracy 79.58185053380782 | Test Accuracy 79.14216534071255\n",
      "Epoch 53 | Train loss 0.6826710104942322\n",
      "Train Accuracy 76.54211150652431 | Test Accuracy 75.51020408163265\n",
      "Epoch 54 | Train loss 0.38218897581100464\n",
      "Train Accuracy 77.46144721233689 | Test Accuracy 76.54790729851263\n",
      "Epoch 55 | Train loss 0.4656207263469696\n",
      "Train Accuracy 79.75978647686833 | Test Accuracy 79.52265652023522\n",
      "Epoch 56 | Train loss 0.3567639887332916\n",
      "Train Accuracy 79.53736654804271 | Test Accuracy 79.03839501902455\n",
      "Epoch 57 | Train loss 0.4789871871471405\n",
      "Train Accuracy 79.87841043890866 | Test Accuracy 80.07609823590454\n",
      "Epoch 58 | Train loss 0.47413310408592224\n",
      "Train Accuracy 79.59667852906287 | Test Accuracy 79.14216534071255\n",
      "Epoch 59 | Train loss 0.4677791893482208\n",
      "Train Accuracy 78.929418742586 | Test Accuracy 78.4503631961259\n",
      "Epoch 60 | Train loss 0.4257068932056427\n",
      "Train Accuracy 80.1453143534994 | Test Accuracy 80.17986855759253\n",
      "Epoch 61 | Train loss 0.4329354166984558\n",
      "Train Accuracy 80.02669039145907 | Test Accuracy 79.62642684192322\n",
      "Epoch 62 | Train loss 0.3886326849460602\n",
      "Train Accuracy 80.10083036773428 | Test Accuracy 80.07609823590454\n",
      "Epoch 63 | Train loss 0.47122448682785034\n",
      "Train Accuracy 79.22597864768683 | Test Accuracy 78.58872362504323\n",
      "Epoch 64 | Train loss 0.46010151505470276\n",
      "Train Accuracy 79.53736654804271 | Test Accuracy 78.9692148045659\n",
      "Epoch 65 | Train loss 0.5391058325767517\n",
      "Train Accuracy 79.2111506524318 | Test Accuracy 78.83085437564856\n",
      "Epoch 66 | Train loss 0.5567371845245361\n",
      "Train Accuracy 80.18979833926453 | Test Accuracy 79.31511587685922\n",
      "Epoch 67 | Train loss 0.4642820656299591\n",
      "Train Accuracy 80.10083036773428 | Test Accuracy 80.00691802144587\n",
      "Epoch 68 | Train loss 0.47300985455513\n",
      "Train Accuracy 80.44187425860024 | Test Accuracy 80.45658941542719\n",
      "Epoch 69 | Train loss 0.5810120105743408\n",
      "Train Accuracy 75.77105575326216 | Test Accuracy 75.26807333102732\n",
      "Epoch 70 | Train loss 0.4601663649082184\n",
      "Train Accuracy 80.45670225385528 | Test Accuracy 80.0415081286752\n",
      "Epoch 71 | Train loss 0.3766123652458191\n",
      "Train Accuracy 76.34934756820878 | Test Accuracy 75.37184365271531\n",
      "Epoch 72 | Train loss 0.39296063780784607\n",
      "Train Accuracy 80.30842230130486 | Test Accuracy 79.45347630577655\n",
      "Epoch 73 | Train loss 0.40420255064964294\n",
      "Train Accuracy 80.79774614472124 | Test Accuracy 80.56035973711518\n",
      "Epoch 74 | Train loss 0.4879652261734009\n",
      "Train Accuracy 80.64946619217082 | Test Accuracy 80.73331027326185\n",
      "Epoch 75 | Train loss 0.4228499233722687\n",
      "Train Accuracy 80.63463819691577 | Test Accuracy 80.49117952265652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 | Train loss 0.39618417620658875\n",
      "Train Accuracy 80.99051008303677 | Test Accuracy 80.66413005880318\n",
      "Epoch 77 | Train loss 0.5435583591461182\n",
      "Train Accuracy 79.49288256227757 | Test Accuracy 79.00380491179523\n",
      "Epoch 78 | Train loss 0.4749833941459656\n",
      "Train Accuracy 79.86358244365361 | Test Accuracy 79.31511587685922\n",
      "Epoch 79 | Train loss 0.45353591442108154\n",
      "Train Accuracy 81.00533807829181 | Test Accuracy 80.97544102386718\n",
      "Epoch 80 | Train loss 0.4578724801540375\n",
      "Train Accuracy 80.66429418742585 | Test Accuracy 80.07609823590454\n",
      "Epoch 81 | Train loss 0.5014990568161011\n",
      "Train Accuracy 72.84994068801898 | Test Accuracy 72.15496368038741\n",
      "Epoch 82 | Train loss 0.4941251873970032\n",
      "Train Accuracy 80.20462633451957 | Test Accuracy 80.00691802144587\n",
      "Epoch 83 | Train loss 0.5103566646575928\n",
      "Train Accuracy 80.99051008303677 | Test Accuracy 80.83708059494984\n",
      "Epoch 84 | Train loss 0.43231886625289917\n",
      "Train Accuracy 80.84223013048636 | Test Accuracy 81.04462123832585\n",
      "Epoch 85 | Train loss 0.40367019176483154\n",
      "Train Accuracy 81.27224199288257 | Test Accuracy 80.87167070217917\n",
      "Epoch 86 | Train loss 0.40449991822242737\n",
      "Train Accuracy 81.15361803084224 | Test Accuracy 80.49117952265652\n",
      "Epoch 87 | Train loss 0.4676642119884491\n",
      "Train Accuracy 80.3232502965599 | Test Accuracy 79.73019716361121\n",
      "Epoch 88 | Train loss 0.42234352231025696\n",
      "Train Accuracy 81.02016607354685 | Test Accuracy 81.04462123832585\n",
      "Epoch 89 | Train loss 0.46977442502975464\n",
      "Train Accuracy 80.41221826809016 | Test Accuracy 79.9377378069872\n",
      "Epoch 90 | Train loss 0.47235965728759766\n",
      "Train Accuracy 81.30189798339265 | Test Accuracy 81.14839156001383\n",
      "Epoch 91 | Train loss 0.42798101902008057\n",
      "Train Accuracy 80.87188612099644 | Test Accuracy 80.2490487720512\n",
      "Epoch 92 | Train loss 0.35617774724960327\n",
      "Train Accuracy 80.79774614472124 | Test Accuracy 80.2490487720512\n",
      "Epoch 93 | Train loss 0.49327534437179565\n",
      "Train Accuracy 81.52431791221827 | Test Accuracy 81.01003113109651\n",
      "Epoch 94 | Train loss 0.47513312101364136\n",
      "Train Accuracy 81.43534994068801 | Test Accuracy 81.1138014527845\n",
      "Epoch 95 | Train loss 0.44381943345069885\n",
      "Train Accuracy 77.96559905100831 | Test Accuracy 77.37806987201661\n",
      "Epoch 96 | Train loss 0.5148690938949585\n",
      "Train Accuracy 81.12396204033215 | Test Accuracy 80.62953995157385\n",
      "Epoch 97 | Train loss 0.5047462582588196\n",
      "Train Accuracy 80.72360616844603 | Test Accuracy 80.66413005880318\n",
      "Epoch 98 | Train loss 0.38520100712776184\n",
      "Train Accuracy 81.56880189798339 | Test Accuracy 81.56347284676583\n",
      "Epoch 99 | Train loss 0.44031476974487305\n",
      "Train Accuracy 80.81257413997628 | Test Accuracy 80.87167070217917\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "bestModel, highestAcc = None, 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    loss, _ = train(train_loader, [0, 1, 2, 3])\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model, train_loader, [0, 1, 2, 3]), check_accuracy(model, test_loader, [0, 1, 2, 3])\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "    \n",
    "    if test_acc > highestAcc:\n",
    "        highestAcc = test_acc\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square\n",
    "import networkx as nx\n",
    "square = nx.Graph()\n",
    "square.add_edge(0, 1)\n",
    "square.add_edge(1, 3)\n",
    "square.add_edge(0, 2)\n",
    "square.add_edge(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square with cross bar (rigid)\n",
    "import networkx as nx\n",
    "square_bar = nx.Graph()\n",
    "square_bar.add_edge(0, 1)\n",
    "square_bar.add_edge(1, 3)\n",
    "square_bar.add_edge(0, 2)\n",
    "square_bar.add_edge(2, 3)\n",
    "square_bar.add_edge(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangle\n",
    "import networkx as nx\n",
    "triangle = nx.Graph()\n",
    "triangle.add_edge(0, 1)\n",
    "triangle.add_edge(0, 2)\n",
    "triangle.add_edge(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentagon\n",
    "import networkx as nx\n",
    "pentagon = nx.Graph()\n",
    "pentagon.add_edge(0, 1)\n",
    "pentagon.add_edge(1, 3)\n",
    "pentagon.add_edge(3, 4)\n",
    "pentagon.add_edge(4, 2)\n",
    "pentagon.add_edge(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no triangle and rigid\n",
    "rigid = nx.Graph()\n",
    "rigid.add_edge(0, 1)\n",
    "rigid.add_edge(0, 2)\n",
    "rigid.add_edge(0, 4)\n",
    "rigid.add_edge(1, 2)\n",
    "rigid.add_edge(1, 5)\n",
    "rigid.add_edge(2, 3)\n",
    "rigid.add_edge(3, 4)\n",
    "rigid.add_edge(3, 5)\n",
    "rigid.add_edge(4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "bestModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_problems = [square, square_bar, triangle, pentagon, rigid]\n",
    "labels = [1, 0, 0, 1, 0]\n",
    "\n",
    "for index, toy_problem in enumerate(toy_problems):\n",
    "    graph_as_data = from_networkx(toy_problem)\n",
    "    graph_as_data.x = generate_feature_vector(toy_problem)\n",
    "#     print(generate_feature_vector(toy_problem))\n",
    "#     graph_as_data.label = labels[index]\n",
    "    validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)\n",
    "    for batch in validation_set:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "#             print(batch.x[:, [0, 1, 2, 3]])\n",
    "#             print(\"******\")\n",
    "#             print(batch.edge_index)\n",
    "#             print(\"******\")\n",
    "#             print(batch.batch)\n",
    "#             print(\"******\")\n",
    "            pred = model(batch.x[:, [0, 1, 2, 3]], batch.edge_index, batch.batch)\n",
    "            print(pred)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the bad examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_to_generate_bad_data = DataLoader(test_data, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_generate_bad_data = DataLoader(train_data, batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_to_generate_bad_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model, train_to_generate_bad_data, [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectly_predicted_flexible_graphs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectly_predicted_rigid_graphs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(incorrectly_predicted_flexible_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(incorrectly_predicted_rigid_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model, test_to_generate_bad_data, [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"incorrectly-predicted.pkl.gz\"\n",
    "with gzip.open(output_file, 'wb') as f:\n",
    "    pickle.dump((incorrectly_predicted_rigid_graphs, incorrectly_predicted_flexible_graphs), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_to_generate_bad_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader, features_to_use):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "            pred = torch.squeeze(pred)\n",
    "            y = batch.label\n",
    "            predictions = (pred > 0.5).long() \n",
    "            num_correct += (predictions == y).sum() \n",
    "            if not (predictions == y).sum():\n",
    "                graph = to_networkx(batch, to_undirected = True)\n",
    "                if y[0] == 0:\n",
    "                    incorrectly_predicted_rigid_graphs.append(graph)\n",
    "                elif y[0] == 1:\n",
    "                    incorrectly_predicted_flexible_graphs.append(graph)\n",
    "                \n",
    "            num_samples += 1\n",
    "            \n",
    "    return float(num_correct)/float(num_samples)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.training)\n",
    "model.train()\n",
    "print(model.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rigid_data, not_rigid_data = [], []\n",
    "stats = {}\n",
    "stats_considered = {}\n",
    "prev_graphs = []\n",
    "\n",
    "stats_wrong = {}\n",
    "\n",
    "num_nodes = 30\n",
    "for p in np.arange(0.01, 0.3, 0.01):\n",
    "    stats[p] = 0\n",
    "    stats_wrong[p] = 0\n",
    "    for num_graphs in range(1000):\n",
    "        G = nx.erdos_renyi_graph(num_nodes, p)        \n",
    "        l = lattice()\n",
    "        num_edges = 0\n",
    "\n",
    "        for (u, v) in G.edges():\n",
    "            if l.add_bond(u, v):\n",
    "                num_edges += 1\n",
    "\n",
    "        label = 1\n",
    "        rigid = False\n",
    "        if num_edges >= (num_nodes * 2) - 3: # rigid \n",
    "            rigid = True\n",
    "            stats[p] += 1\n",
    "            label = 0\n",
    "\n",
    "        graph_as_data = from_networkx(G)\n",
    "        graph_as_data.x = generate_feature_vector(G)\n",
    "        validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)\n",
    "        for batch in validation_set:\n",
    "            pred = model(batch.x[:, [0]], batch.edge_index, batch.batch)\n",
    "            pred_label = 1\n",
    "            if (pred[0][0][0] < 0.5):\n",
    "                pred_label = 0\n",
    "                \n",
    "            if pred_label != label:\n",
    "                print(pred[0][0][0] , \" \", label)\n",
    "                stats_wrong[p] += 1\n",
    "                print(\"wrong: , with number of edges: \" , G.number_of_edges(), \" \", num_edges)\n",
    "                \n",
    "    print(stats[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stats_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(list(stats_wrong.keys()), stats_wrong.values(), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(stats_wrong.values())) / (len(stats_wrong) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stats_wrong) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_as_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rigid_data, not_rigid_data = [], []\n",
    "stats = {}\n",
    "prev_graphs = []\n",
    "\n",
    "num_nodes = 30\n",
    "for p in np.arange(0.01, 0.3, 0.01):\n",
    "    stats[p] = 0\n",
    "    for num_graphs in range(1000):\n",
    "        G = nx.erdos_renyi_graph(num_nodes, p)\n",
    "        l = lattice()\n",
    "        num_edges = 0\n",
    "\n",
    "        for (u, v) in G.edges():\n",
    "            if l.add_bond(u, v):\n",
    "                num_edges += 1\n",
    "\n",
    "        rigid = False\n",
    "        if num_edges >= (num_nodes * 2) - 3: # rigid \n",
    "            rigid_data.append(G)\n",
    "            stats[p] += 1\n",
    "        else:\n",
    "            not_rigid_data.append(G)\n",
    "\n",
    "        prev_graphs.append(G)\n",
    "        \n",
    "    print(stats[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rigid_data_1_wrong, not_rigid_data_1_wrong = [], []\n",
    "stats_wrong = {}\n",
    "stats_wrong_cum = {}\n",
    "prev_graphs = []\n",
    "\n",
    "num_nodes = 30\n",
    "for num_edges in range(57, 200):\n",
    "    model.eval()\n",
    "    stats_wrong[num_edges] = 0\n",
    "    for num_graphs in range(10):\n",
    "        G = generate_rigid_nodes_edges(num_nodes, num_edges)\n",
    "        \n",
    "#         G = nx.erdos_renyi_graph(num_nodes, p)\n",
    "#         generate_rigid_nodes_edges()\n",
    "        graph_as_data = from_networkx(G)\n",
    "        graph_as_data.x = generate_feature_vector(G)\n",
    "#         graph_as_data.label = labels[index]\n",
    "        validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)\n",
    "        for batch in validation_set:\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch.x[:, [0, 1, 2, 3]], batch.edge_index, batch.batch)\n",
    "                if (pred[0][0][0] > 0.5):\n",
    "                    stats_wrong[num_edges] += 1\n",
    "                    print(\"WRONG\")\n",
    "                \n",
    "\n",
    "print(stats_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in validation_set:\n",
    "    pred = bestModel(batch.x, batch.edge_index, batch.batch)\n",
    "    print(pred[0])\n",
    "    print(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanTestDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_test.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        # processing code here\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        ind = 0\n",
    "        # convert from graph to Data object\n",
    "        for graph in total_laman_data[0]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 1)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        ind = 0\n",
    "        for graph in total_laman_data[1]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 64)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add functionality to support a test dataset\n",
    "TEST_DATA_PATH = \"../data-2d/data/test-dataset-30loc-5std.pkl.gz\"\n",
    "laman_test_set = LamanTestDataset(\"\", TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "laman_test_loader = DataLoader(laman_test_set, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_acc = check_accuracy(model, test_loader, [0, 1, 2, 3])\n",
    "print(f\"Accuracy {random_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate statistics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(square, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(triangle, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(square_bar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate graph correlating clustering coefficient to rigidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_data:\n",
    "    item = to_networkx(item)\n",
    "    print(type(item))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.utils.convert.to_networkx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work: Sahil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_laman_data = None\n",
    "with gzip.open(DATA_PATH, 'r') as f:\n",
    "    total_laman_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graph = total_laman_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sample_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_data = from_networkx(sample_graph)\n",
    "from_data = to_networkx(to_data, to_undirected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(from_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_clustering_coefficient(G):\n",
    "    min_coefficient = 1\n",
    "    for node in G.nodes():\n",
    "        min_coefficient = min(min_coefficient, clustering_coefficient(G, node))\n",
    "        \n",
    "    return min_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sample_graph in enumerate(train_data):\n",
    "    label = sample_graph.label\n",
    "    networkx_sample_graph = to_networkx(sample_graph, to_undirected = True)\n",
    "    print(label, \" \", index, \" \", compute_min_clustering_coefficient(networkx_sample_graph))\n",
    "    \n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_min_clustering_coefficient(from_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_min_clustering_coefficient(triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_just_degree = GIN(num_features=1)\n",
    "print(model_just_degree)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "bestModel, highestAcc = None, 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss, h = train(train_loader, [0, 1])\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model_just_degree, train_loader), check_accuracy(model_just_degree, test_loader)\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "#     scheduler.step(test_acc)\n",
    "    \n",
    "    if test_acc > highestAcc:\n",
    "        highestAcc = test_acc\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_feature_vector(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graph.x[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_gen import generate_rigid_nodes_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/sahiljain/Documents/Fall 2022/Independent Work/reversible-inductive-construction/code/genric/laman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(triangle, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"gcn-model-filtered-data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
