{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing PyG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(G):\n",
    "    x = torch.randn(G.number_of_nodes(), 1)\n",
    "    ind = 0\n",
    "    for node in G.nodes():\n",
    "        x[ind][0] = G.degree[node]\n",
    "        ind += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        # processing code here\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        ind = 0\n",
    "        # convert from graph to Data object\n",
    "        for graph in total_laman_data[0]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 1)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        ind = 0\n",
    "        for graph in total_laman_data[1]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 64)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data-2d/data/4096-20-4-entries-med.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_data = LamanDataset(\"\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "proportions = [.8, .2]\n",
    "lengths = [int(p * len(laman_data)) for p in proportions]\n",
    "lengths[-1] = len(laman_data) - sum(lengths[:-1])\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_data, test_data = random_split(laman_data, lengths, generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = 256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches:  13\n",
      "Number of test batches:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches: \", len(train_loader))\n",
    "print(\"Number of test batches: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 19530], x=[5215, 1], label=[256], num_nodes=5215, batch=[5215], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gin.gin import GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN(\n",
      "  (conv1): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv2): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv3): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv4): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv5): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv6): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv7): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (lin1): Linear(in_features=112, out_features=112, bias=True)\n",
      "  (lin2): Linear(in_features=112, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  16561\n"
     ]
    }
   ],
   "source": [
    "model = GIN(num_features=1)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    ind = 0\n",
    "    for batch in data:\n",
    "        optimizer.zero_grad()\n",
    "        pred, embedding = model(batch.x, batch.edge_index, batch.batch)\n",
    "        pred = torch.squeeze(pred)\n",
    "        loss = loss_fn(pred.float(), batch.label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ind += 1\n",
    "\n",
    "    return loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pred, embedding = model(batch.x, batch.edge_index, batch.batch)\n",
    "            pred = torch.squeeze(pred)\n",
    "            y = batch.label\n",
    "            predictions = (pred > 0.5).long() \n",
    "            num_correct += (predictions == y).sum() \n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "    return float(num_correct)/float(num_samples)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Train loss 0.5209265947341919\n",
      "Train Accuracy 65.35409035409036 | Test Accuracy 67.92682926829269\n",
      "Epoch 1 | Train loss 0.5815510153770447\n",
      "Train Accuracy 73.26007326007326 | Test Accuracy 76.82926829268293\n",
      "Epoch 2 | Train loss 0.5442524552345276\n",
      "Train Accuracy 70.26862026862027 | Test Accuracy 70.73170731707317\n",
      "Epoch 3 | Train loss 0.4728721082210541\n",
      "Train Accuracy 75.54945054945054 | Test Accuracy 77.07317073170732\n",
      "Epoch 4 | Train loss 0.4501432776451111\n",
      "Train Accuracy 76.12942612942612 | Test Accuracy 77.5609756097561\n",
      "Epoch 5 | Train loss 0.44835999608039856\n",
      "Train Accuracy 78.08302808302808 | Test Accuracy 79.39024390243902\n",
      "Epoch 6 | Train loss 0.47201594710350037\n",
      "Train Accuracy 78.75457875457876 | Test Accuracy 79.75609756097562\n",
      "Epoch 7 | Train loss 0.3701326251029968\n",
      "Train Accuracy 78.32722832722833 | Test Accuracy 79.8780487804878\n",
      "Epoch 8 | Train loss 0.4398428201675415\n",
      "Train Accuracy 78.41880341880342 | Test Accuracy 81.70731707317073\n",
      "Epoch 9 | Train loss 0.40796875953674316\n",
      "Train Accuracy 79.60927960927961 | Test Accuracy 80.48780487804879\n",
      "Epoch 10 | Train loss 0.3981233835220337\n",
      "Train Accuracy 80.12820512820514 | Test Accuracy 81.82926829268293\n",
      "Epoch 11 | Train loss 0.4020434021949768\n",
      "Train Accuracy 80.67765567765568 | Test Accuracy 81.95121951219512\n",
      "Epoch 12 | Train loss 0.39326393604278564\n",
      "Train Accuracy 80.31135531135531 | Test Accuracy 83.04878048780488\n",
      "Epoch 13 | Train loss 0.46981915831565857\n",
      "Train Accuracy 76.8925518925519 | Test Accuracy 77.19512195121952\n",
      "Epoch 14 | Train loss 0.4041878283023834\n",
      "Train Accuracy 75.39682539682539 | Test Accuracy 79.51219512195122\n",
      "Epoch 15 | Train loss 0.45221200585365295\n",
      "Train Accuracy 80.64713064713065 | Test Accuracy 83.65853658536585\n",
      "Epoch 16 | Train loss 0.39586174488067627\n",
      "Train Accuracy 82.23443223443223 | Test Accuracy 83.53658536585365\n",
      "Epoch 17 | Train loss 0.3331204652786255\n",
      "Train Accuracy 81.92918192918192 | Test Accuracy 82.1951219512195\n",
      "Epoch 18 | Train loss 0.41515615582466125\n",
      "Train Accuracy 81.5934065934066 | Test Accuracy 84.51219512195122\n",
      "Epoch 19 | Train loss 0.3651907444000244\n",
      "Train Accuracy 79.85347985347985 | Test Accuracy 83.65853658536585\n",
      "Epoch 20 | Train loss 0.32502585649490356\n",
      "Train Accuracy 83.3028083028083 | Test Accuracy 86.21951219512195\n",
      "Epoch 21 | Train loss 0.382140576839447\n",
      "Train Accuracy 83.76068376068376 | Test Accuracy 86.09756097560975\n",
      "Epoch 22 | Train loss 0.3822420537471771\n",
      "Train Accuracy 85.65323565323565 | Test Accuracy 85.24390243902438\n",
      "Epoch 23 | Train loss 0.33761313557624817\n",
      "Train Accuracy 86.01953601953602 | Test Accuracy 86.34146341463415\n",
      "Epoch 24 | Train loss 0.45553460717201233\n",
      "Train Accuracy 86.32478632478633 | Test Accuracy 87.4390243902439\n",
      "Epoch 25 | Train loss 0.3296988606452942\n",
      "Train Accuracy 87.11843711843711 | Test Accuracy 86.82926829268293\n",
      "Epoch 26 | Train loss 0.3182147145271301\n",
      "Train Accuracy 88.36996336996337 | Test Accuracy 88.41463414634147\n",
      "Epoch 27 | Train loss 0.3962289094924927\n",
      "Train Accuracy 87.82051282051282 | Test Accuracy 88.41463414634147\n",
      "Epoch 28 | Train loss 0.3727089762687683\n",
      "Train Accuracy 86.01953601953602 | Test Accuracy 85.24390243902438\n",
      "Epoch 29 | Train loss 0.2807264029979706\n",
      "Train Accuracy 90.07936507936508 | Test Accuracy 90.48780487804878\n",
      "Epoch 30 | Train loss 0.24759045243263245\n",
      "Train Accuracy 90.56776556776556 | Test Accuracy 89.8780487804878\n",
      "Epoch 31 | Train loss 0.29262542724609375\n",
      "Train Accuracy 90.93406593406593 | Test Accuracy 91.09756097560975\n",
      "Epoch 32 | Train loss 0.22662222385406494\n",
      "Train Accuracy 91.63614163614163 | Test Accuracy 90.97560975609757\n",
      "Epoch 33 | Train loss 0.22456534206867218\n",
      "Train Accuracy 84.89010989010988 | Test Accuracy 89.26829268292683\n",
      "Epoch 34 | Train loss 0.3055083453655243\n",
      "Train Accuracy 78.41880341880342 | Test Accuracy 79.75609756097562\n",
      "Epoch 35 | Train loss 0.3469483554363251\n",
      "Train Accuracy 85.92796092796092 | Test Accuracy 89.02439024390245\n",
      "Epoch 36 | Train loss 0.2768597900867462\n",
      "Train Accuracy 88.82783882783882 | Test Accuracy 90.2439024390244\n",
      "Epoch 37 | Train loss 0.27613040804862976\n",
      "Train Accuracy 90.93406593406593 | Test Accuracy 91.46341463414635\n",
      "Epoch 38 | Train loss 0.23027867078781128\n",
      "Train Accuracy 92.94871794871796 | Test Accuracy 92.4390243902439\n",
      "Epoch 39 | Train loss 0.27750322222709656\n",
      "Train Accuracy 78.6019536019536 | Test Accuracy 79.26829268292683\n",
      "Epoch 40 | Train loss 0.2634551227092743\n",
      "Train Accuracy 89.83516483516483 | Test Accuracy 90.97560975609757\n",
      "Epoch 41 | Train loss 0.3258920907974243\n",
      "Train Accuracy 90.47619047619048 | Test Accuracy 90.48780487804878\n",
      "Epoch 42 | Train loss 0.23190009593963623\n",
      "Train Accuracy 92.52136752136752 | Test Accuracy 93.04878048780488\n",
      "Epoch 43 | Train loss 0.25031256675720215\n",
      "Train Accuracy 90.35409035409036 | Test Accuracy 91.58536585365854\n",
      "Epoch 44 | Train loss 0.20575577020645142\n",
      "Train Accuracy 94.38339438339437 | Test Accuracy 94.6341463414634\n",
      "Epoch 45 | Train loss 0.23206418752670288\n",
      "Train Accuracy 94.84126984126983 | Test Accuracy 94.8780487804878\n",
      "Epoch 46 | Train loss 0.15532635152339935\n",
      "Train Accuracy 93.74236874236874 | Test Accuracy 93.53658536585367\n",
      "Epoch 47 | Train loss 0.2409783899784088\n",
      "Train Accuracy 94.74969474969474 | Test Accuracy 93.53658536585367\n",
      "Epoch 48 | Train loss 0.20812614262104034\n",
      "Train Accuracy 93.52869352869352 | Test Accuracy 94.39024390243902\n",
      "Epoch 49 | Train loss 0.14982837438583374\n",
      "Train Accuracy 96.67277167277167 | Test Accuracy 96.82926829268293\n",
      "Epoch 50 | Train loss 0.2641628086566925\n",
      "Train Accuracy 94.74969474969474 | Test Accuracy 93.78048780487805\n",
      "Epoch 51 | Train loss 0.22147949039936066\n",
      "Train Accuracy 94.93284493284493 | Test Accuracy 95.36585365853658\n",
      "Epoch 52 | Train loss 0.1881079077720642\n",
      "Train Accuracy 84.15750915750915 | Test Accuracy 82.68292682926828\n",
      "Epoch 53 | Train loss 0.13336721062660217\n",
      "Train Accuracy 98.5958485958486 | Test Accuracy 98.04878048780488\n",
      "Epoch 54 | Train loss 0.1398756504058838\n",
      "Train Accuracy 97.64957264957265 | Test Accuracy 97.1951219512195\n",
      "Epoch 55 | Train loss 0.17003397643566132\n",
      "Train Accuracy 94.96336996336996 | Test Accuracy 93.65853658536587\n",
      "Epoch 56 | Train loss 0.10650264471769333\n",
      "Train Accuracy 98.96214896214897 | Test Accuracy 98.90243902439025\n",
      "Epoch 57 | Train loss 0.1442413181066513\n",
      "Train Accuracy 98.84004884004884 | Test Accuracy 98.41463414634146\n",
      "Epoch 58 | Train loss 0.09323179721832275\n",
      "Train Accuracy 99.2979242979243 | Test Accuracy 99.02439024390245\n",
      "Epoch 59 | Train loss 0.10014375299215317\n",
      "Train Accuracy 99.60317460317461 | Test Accuracy 99.63414634146342\n",
      "Epoch 60 | Train loss 0.11908736824989319\n",
      "Train Accuracy 94.23076923076923 | Test Accuracy 94.51219512195121\n",
      "Epoch 61 | Train loss 0.14313508570194244\n",
      "Train Accuracy 99.48107448107449 | Test Accuracy 99.63414634146342\n",
      "Epoch 62 | Train loss 0.08206626772880554\n",
      "Train Accuracy 99.20634920634922 | Test Accuracy 99.14634146341463\n",
      "Epoch 63 | Train loss 0.08326523005962372\n",
      "Train Accuracy 93.68131868131869 | Test Accuracy 94.14634146341463\n",
      "Epoch 64 | Train loss 0.06623084098100662\n",
      "Train Accuracy 99.9084249084249 | Test Accuracy 99.7560975609756\n",
      "Epoch 65 | Train loss 0.07869008183479309\n",
      "Train Accuracy 99.9084249084249 | Test Accuracy 100.0\n",
      "Epoch 66 | Train loss 0.05717853456735611\n",
      "Train Accuracy 99.84737484737485 | Test Accuracy 99.8780487804878\n",
      "Epoch 67 | Train loss 0.05480945482850075\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 68 | Train loss 0.7817222476005554\n",
      "Train Accuracy 80.95238095238095 | Test Accuracy 85.24390243902438\n",
      "Epoch 69 | Train loss 0.2606652081012726\n",
      "Train Accuracy 97.61904761904762 | Test Accuracy 97.5609756097561\n",
      "Epoch 70 | Train loss 0.10347403585910797\n",
      "Train Accuracy 99.35897435897436 | Test Accuracy 99.63414634146342\n",
      "Epoch 71 | Train loss 0.09587536752223969\n",
      "Train Accuracy 99.78632478632478 | Test Accuracy 99.7560975609756\n",
      "Epoch 72 | Train loss 0.07504043728113174\n",
      "Train Accuracy 99.96947496947497 | Test Accuracy 100.0\n",
      "Epoch 73 | Train loss 0.07383870333433151\n",
      "Train Accuracy 99.66422466422466 | Test Accuracy 99.63414634146342\n",
      "Epoch 74 | Train loss 0.05638888478279114\n",
      "Train Accuracy 98.50427350427351 | Test Accuracy 98.65853658536585\n",
      "Epoch 75 | Train loss 0.0653795525431633\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.63414634146342\n",
      "Epoch 76 | Train loss 0.05533267930150032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 99.81684981684981 | Test Accuracy 99.8780487804878\n",
      "Epoch 77 | Train loss 0.04423908144235611\n",
      "Train Accuracy 99.9084249084249 | Test Accuracy 100.0\n",
      "Epoch 78 | Train loss 0.04294076934456825\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.7560975609756\n",
      "Epoch 79 | Train loss 0.052222054451704025\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 80 | Train loss 0.048643626272678375\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 81 | Train loss 0.03910990059375763\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 82 | Train loss 0.03098950907588005\n",
      "Train Accuracy 99.38949938949939 | Test Accuracy 99.39024390243902\n",
      "Epoch 83 | Train loss 0.029945749789476395\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 84 | Train loss 0.03203766793012619\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 85 | Train loss 0.03246258944272995\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 86 | Train loss 0.030716385692358017\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 87 | Train loss 0.030521629378199577\n",
      "Train Accuracy 99.96947496947497 | Test Accuracy 99.8780487804878\n",
      "Epoch 88 | Train loss 0.03357842192053795\n",
      "Train Accuracy 99.96947496947497 | Test Accuracy 99.8780487804878\n",
      "Epoch 89 | Train loss 0.025346538051962852\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 90 | Train loss 0.03179779648780823\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 91 | Train loss 0.025854377076029778\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 92 | Train loss 0.02145816758275032\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 93 | Train loss 0.01926378533244133\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 94 | Train loss 0.022403836250305176\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 95 | Train loss 0.01611495576798916\n",
      "Train Accuracy 100.0 | Test Accuracy 99.8780487804878\n",
      "Epoch 96 | Train loss 0.017489904537796974\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 97 | Train loss 0.021847423166036606\n",
      "Train Accuracy 100.0 | Test Accuracy 99.7560975609756\n",
      "Epoch 98 | Train loss 0.04457886889576912\n",
      "Train Accuracy 100.0 | Test Accuracy 100.0\n",
      "Epoch 99 | Train loss 2.9192497730255127\n",
      "Train Accuracy 89.77411477411478 | Test Accuracy 91.58536585365854\n",
      "Epoch 100 | Train loss 0.17777401208877563\n",
      "Train Accuracy 80.03663003663004 | Test Accuracy 81.09756097560977\n",
      "Epoch 101 | Train loss 0.2542325556278229\n",
      "Train Accuracy 86.01953601953602 | Test Accuracy 87.8048780487805\n",
      "Epoch 102 | Train loss 0.16130870580673218\n",
      "Train Accuracy 93.95604395604396 | Test Accuracy 95.60975609756098\n",
      "Epoch 103 | Train loss 0.18962033092975616\n",
      "Train Accuracy 91.14774114774114 | Test Accuracy 91.34146341463415\n",
      "Epoch 104 | Train loss 0.19652177393436432\n",
      "Train Accuracy 97.37484737484738 | Test Accuracy 98.04878048780488\n",
      "Epoch 105 | Train loss 0.10313761234283447\n",
      "Train Accuracy 95.32967032967034 | Test Accuracy 95.60975609756098\n",
      "Epoch 106 | Train loss 0.07918626815080643\n",
      "Train Accuracy 98.47374847374849 | Test Accuracy 99.14634146341463\n",
      "Epoch 107 | Train loss 0.06745333969593048\n",
      "Train Accuracy 98.9010989010989 | Test Accuracy 99.39024390243902\n",
      "Epoch 108 | Train loss 0.07816585153341293\n",
      "Train Accuracy 97.95482295482294 | Test Accuracy 98.53658536585365\n",
      "Epoch 109 | Train loss 0.053044259548187256\n",
      "Train Accuracy 98.5958485958486 | Test Accuracy 99.02439024390245\n",
      "Epoch 110 | Train loss 0.034328073263168335\n",
      "Train Accuracy 97.37484737484738 | Test Accuracy 97.3170731707317\n",
      "Epoch 111 | Train loss 0.04273354634642601\n",
      "Train Accuracy 99.72527472527473 | Test Accuracy 99.63414634146342\n",
      "Epoch 112 | Train loss 0.0617557056248188\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.51219512195122\n",
      "Epoch 113 | Train loss 0.14787828922271729\n",
      "Train Accuracy 93.95604395604396 | Test Accuracy 95.36585365853658\n",
      "Epoch 114 | Train loss 0.10031739622354507\n",
      "Train Accuracy 95.08547008547008 | Test Accuracy 95.60975609756098\n",
      "Epoch 115 | Train loss 0.04457653686404228\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.7560975609756\n",
      "Epoch 116 | Train loss 0.05045420676469803\n",
      "Train Accuracy 99.54212454212454 | Test Accuracy 99.51219512195122\n",
      "Epoch 117 | Train loss 0.027283145114779472\n",
      "Train Accuracy 99.78632478632478 | Test Accuracy 99.8780487804878\n",
      "Epoch 118 | Train loss 0.029406262561678886\n",
      "Train Accuracy 99.60317460317461 | Test Accuracy 99.63414634146342\n",
      "Epoch 119 | Train loss 0.0270090289413929\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.8780487804878\n",
      "Epoch 120 | Train loss 0.04109608754515648\n",
      "Train Accuracy 99.48107448107449 | Test Accuracy 99.39024390243902\n",
      "Epoch 121 | Train loss 0.04966859146952629\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.7560975609756\n",
      "Epoch 122 | Train loss 0.04540485888719559\n",
      "Train Accuracy 99.78632478632478 | Test Accuracy 99.8780487804878\n",
      "Epoch 123 | Train loss 1.1494200229644775\n",
      "Train Accuracy 99.02319902319903 | Test Accuracy 98.78048780487805\n",
      "Epoch 124 | Train loss 0.10540025681257248\n",
      "Train Accuracy 98.22954822954823 | Test Accuracy 99.14634146341463\n",
      "Epoch 125 | Train loss 0.10799653083086014\n",
      "Train Accuracy 99.11477411477412 | Test Accuracy 99.51219512195122\n",
      "Epoch 126 | Train loss 0.05452984943985939\n",
      "Train Accuracy 99.23687423687424 | Test Accuracy 99.14634146341463\n",
      "Epoch 127 | Train loss 0.061061471700668335\n",
      "Train Accuracy 98.80952380952381 | Test Accuracy 99.14634146341463\n",
      "Epoch 128 | Train loss 0.07468051463365555\n",
      "Train Accuracy 98.53479853479854 | Test Accuracy 98.65853658536585\n",
      "Epoch 129 | Train loss 0.04435034468770027\n",
      "Train Accuracy 99.45054945054946 | Test Accuracy 99.63414634146342\n",
      "Epoch 130 | Train loss 0.05447785556316376\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.8780487804878\n",
      "Epoch 131 | Train loss 0.044565580785274506\n",
      "Train Accuracy 99.66422466422466 | Test Accuracy 99.8780487804878\n",
      "Epoch 132 | Train loss 0.04178085923194885\n",
      "Train Accuracy 99.38949938949939 | Test Accuracy 99.39024390243902\n",
      "Epoch 133 | Train loss 0.04384628310799599\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.8780487804878\n",
      "Epoch 134 | Train loss 0.046855684369802475\n",
      "Train Accuracy 99.60317460317461 | Test Accuracy 99.8780487804878\n",
      "Epoch 135 | Train loss 0.02798704244196415\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.63414634146342\n",
      "Epoch 136 | Train loss 0.030235329642891884\n",
      "Train Accuracy 99.57264957264957 | Test Accuracy 99.7560975609756\n",
      "Epoch 137 | Train loss 0.02338080294430256\n",
      "Train Accuracy 99.45054945054946 | Test Accuracy 99.39024390243902\n",
      "Epoch 138 | Train loss 0.026376070454716682\n",
      "Train Accuracy 99.48107448107449 | Test Accuracy 99.39024390243902\n",
      "Epoch 139 | Train loss 0.031199678778648376\n",
      "Train Accuracy 99.42002442002442 | Test Accuracy 99.39024390243902\n",
      "Epoch 140 | Train loss 0.032858531922101974\n",
      "Train Accuracy 99.57264957264957 | Test Accuracy 99.7560975609756\n",
      "Epoch 141 | Train loss 0.024955885484814644\n",
      "Train Accuracy 99.57264957264957 | Test Accuracy 99.7560975609756\n",
      "Epoch 142 | Train loss 0.016528654843568802\n",
      "Train Accuracy 99.6947496947497 | Test Accuracy 99.8780487804878\n",
      "Epoch 143 | Train loss 0.03773663938045502\n",
      "Train Accuracy 99.84737484737485 | Test Accuracy 99.8780487804878\n",
      "Epoch 144 | Train loss 0.0348285436630249\n",
      "Train Accuracy 99.78632478632478 | Test Accuracy 99.8780487804878\n",
      "Epoch 145 | Train loss 0.021085748448967934\n",
      "Train Accuracy 99.48107448107449 | Test Accuracy 99.39024390243902\n",
      "Epoch 146 | Train loss 0.026515178382396698\n",
      "Train Accuracy 99.6947496947497 | Test Accuracy 99.8780487804878\n",
      "Epoch 147 | Train loss 0.023613562807440758\n",
      "Train Accuracy 99.93894993894995 | Test Accuracy 100.0\n",
      "Epoch 148 | Train loss 0.018647676333785057\n",
      "Train Accuracy 99.60317460317461 | Test Accuracy 99.63414634146342\n",
      "Epoch 149 | Train loss 0.019898150116205215\n",
      "Train Accuracy 99.84737484737485 | Test Accuracy 99.8780487804878\n",
      "Epoch 150 | Train loss 0.02105643041431904\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.7560975609756\n",
      "Epoch 151 | Train loss 0.02012106031179428\n",
      "Train Accuracy 99.84737484737485 | Test Accuracy 99.8780487804878\n",
      "Epoch 152 | Train loss 0.015000688843429089\n",
      "Train Accuracy 99.87789987789988 | Test Accuracy 100.0\n",
      "Epoch 153 | Train loss 0.021961137652397156\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.8780487804878\n",
      "Epoch 154 | Train loss 0.021478567272424698\n",
      "Train Accuracy 99.60317460317461 | Test Accuracy 99.63414634146342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155 | Train loss 0.01872183009982109\n",
      "Train Accuracy 99.93894993894995 | Test Accuracy 99.8780487804878\n",
      "Epoch 156 | Train loss 0.028369497507810593\n",
      "Train Accuracy 95.66544566544567 | Test Accuracy 96.34146341463415\n",
      "Epoch 157 | Train loss 1.912736177444458\n",
      "Train Accuracy 73.80952380952381 | Test Accuracy 78.53658536585367\n",
      "Epoch 158 | Train loss 0.24430511891841888\n",
      "Train Accuracy 74.60317460317461 | Test Accuracy 79.75609756097562\n",
      "Epoch 159 | Train loss 0.12999315559864044\n",
      "Train Accuracy 88.24786324786325 | Test Accuracy 90.97560975609757\n",
      "Epoch 160 | Train loss 0.1041780561208725\n",
      "Train Accuracy 98.07692307692307 | Test Accuracy 98.41463414634146\n",
      "Epoch 161 | Train loss 0.09229019284248352\n",
      "Train Accuracy 97.89377289377289 | Test Accuracy 98.53658536585365\n",
      "Epoch 162 | Train loss 0.0621691457927227\n",
      "Train Accuracy 98.96214896214897 | Test Accuracy 99.51219512195122\n",
      "Epoch 163 | Train loss 0.053386591374874115\n",
      "Train Accuracy 98.74847374847376 | Test Accuracy 99.02439024390245\n",
      "Epoch 164 | Train loss 0.06455451250076294\n",
      "Train Accuracy 99.17582417582418 | Test Accuracy 99.7560975609756\n",
      "Epoch 165 | Train loss 0.04798354208469391\n",
      "Train Accuracy 98.96214896214897 | Test Accuracy 99.14634146341463\n",
      "Epoch 166 | Train loss 0.08941537141799927\n",
      "Train Accuracy 99.2979242979243 | Test Accuracy 99.39024390243902\n",
      "Epoch 167 | Train loss 0.04425251856446266\n",
      "Train Accuracy 99.26739926739927 | Test Accuracy 99.26829268292683\n",
      "Epoch 168 | Train loss 0.042578794062137604\n",
      "Train Accuracy 99.32844932844932 | Test Accuracy 99.39024390243902\n",
      "Epoch 169 | Train loss 0.059903912246227264\n",
      "Train Accuracy 99.48107448107449 | Test Accuracy 99.63414634146342\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     loss, h \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[71], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(pred)\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39mfloat(), batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m ind \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "for epoch in range(500):\n",
    "    loss, h = train(train_loader)\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model, train_loader), check_accuracy(model, test_loader)\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square\n",
    "import networkx as nx\n",
    "square = nx.Graph()\n",
    "square.add_edge(0, 1)\n",
    "square.add_edge(1, 3)\n",
    "square.add_edge(0, 2)\n",
    "square.add_edge(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square with cross bar (rigid)\n",
    "import networkx as nx\n",
    "square_bar = nx.Graph()\n",
    "square_bar.add_edge(0, 1)\n",
    "square_bar.add_edge(1, 3)\n",
    "square_bar.add_edge(0, 2)\n",
    "square_bar.add_edge(2, 3)\n",
    "square_bar.add_edge(0, 3)\n",
    "square_bar.add_edge(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangle\n",
    "import networkx as nx\n",
    "triangle = nx.Graph()\n",
    "triangle.add_edge(0, 1)\n",
    "triangle.add_edge(0, 2)\n",
    "triangle.add_edge(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentagon\n",
    "import networkx as nx\n",
    "pentagon = nx.Graph()\n",
    "pentagon.add_edge(0, 1)\n",
    "pentagon.add_edge(1, 3)\n",
    "pentagon.add_edge(3, 4)\n",
    "pentagon.add_edge(4, 2)\n",
    "pentagon.add_edge(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_as_data = from_networkx(pentagon)\n",
    "graph_as_data.x = generate_feature_vector(pentagon)\n",
    "graph_as_data.label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 10], num_nodes=5, x=[5, 1], label=0)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_as_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[28.9286]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in validation_set:\n",
    "    pred = model(batch.x, batch.edge_index, batch.batch)\n",
    "    print(pred[0])\n",
    "    print(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = check_accuracy(model, train_loader), check_accuracy(model, test_loader)\n",
    "print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
