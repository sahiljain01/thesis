{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing PyG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_coefficient(G, node):\n",
    "    ns = [n for n in G.neighbors(node)]\n",
    "    if len(ns) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = len(ns) * (len(ns) - 1) / 2\n",
    "    for i in range(0, len(ns)):\n",
    "        for j in range(i+1, len(ns)):\n",
    "            n1, n2 = ns[i], ns[j]\n",
    "            numerator += G.has_edge(n1, n2)\n",
    "    \n",
    "    return numerator / denominator\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(G):\n",
    "    x = torch.randn(G.number_of_nodes(), 2)\n",
    "    ind = 0\n",
    "    for node in G.nodes():\n",
    "        # set node degree as feature\n",
    "        x[ind][0] = G.degree[node]\n",
    "        x[ind][1] = clustering_coefficient(G, node)\n",
    "        ind += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        for ind, graph in enumerate(total_laman_data[0]):\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        for ind, graph in enumerate(total_laman_data[1]):\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data-2d/data/4096-20-4-entries-med.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_data = LamanDataset(\"\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 122], x=[32, 2], label=[1], num_nodes=32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laman_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "proportions = [.6, .4]\n",
    "lengths = [int(p * len(laman_data)) for p in proportions]\n",
    "lengths[-1] = len(laman_data) - sum(lengths[:-1])\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_data, test_data = random_split(laman_data, lengths, generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = 256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches:  10\n",
      "Number of test batches:  7\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches: \", len(train_loader))\n",
    "print(\"Number of test batches: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 19192], x=[5157, 2], label=[256], num_nodes=5157, batch=[5157], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gin.gin import GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN(\n",
      "  (conv1): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=1, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv2): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv3): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv4): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv5): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv6): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv7): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (1): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=2, out_features=2, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (lin1): Linear(in_features=14, out_features=14, bias=True)\n",
      "  (lin2): Linear(in_features=14, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  335\n"
     ]
    }
   ],
   "source": [
    "model = GIN(num_features=1)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=0.001)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', min_lr=1e-6, verbose=True, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, features_to_use):\n",
    "    ind = 0\n",
    "    for batch in data:\n",
    "        optimizer.zero_grad()\n",
    "        pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "        pred = torch.squeeze(pred)\n",
    "        loss = loss_fn(pred.float(), batch.label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ind += 1\n",
    "\n",
    "    return loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader, features_to_use):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "            pred = torch.squeeze(pred)\n",
    "            y = batch.label\n",
    "            predictions = (pred > 0.5).long() \n",
    "            num_correct += (predictions == y).sum() \n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "    return float(num_correct)/float(num_samples)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 | Train loss 0.30869102478027344\n",
      "Train Accuracy 90.72039072039072 | Test Accuracy 90.42098840756559\n",
      "Epoch 1 | Train loss 0.2955162227153778\n",
      "Train Accuracy 91.29019129019129 | Test Accuracy 90.9090909090909\n",
      "Epoch 2 | Train loss 0.23613198101520538\n",
      "Train Accuracy 91.85999185999187 | Test Accuracy 91.2751677852349\n",
      "Epoch 3 | Train loss 0.22741472721099854\n",
      "Train Accuracy 91.41229141229141 | Test Accuracy 91.15314215985357\n",
      "Epoch 4 | Train loss 0.33309268951416016\n",
      "Train Accuracy 91.57509157509158 | Test Accuracy 91.2751677852349\n",
      "Epoch 5 | Train loss 0.26165688037872314\n",
      "Train Accuracy 91.77859177859177 | Test Accuracy 91.33618059792556\n",
      "Epoch 6 | Train loss 0.21904051303863525\n",
      "Train Accuracy 92.22629222629223 | Test Accuracy 91.70225747406955\n",
      "Epoch 7 | Train loss 0.2693001925945282\n",
      "Train Accuracy 92.22629222629223 | Test Accuracy 91.94630872483222\n",
      "Epoch 8 | Train loss 0.2384323924779892\n",
      "Train Accuracy 92.10419210419211 | Test Accuracy 91.70225747406955\n",
      "Epoch 9 | Train loss 0.24637843668460846\n",
      "Train Accuracy 91.53439153439153 | Test Accuracy 91.64124466137889\n",
      "Epoch 10 | Train loss 0.2714560925960541\n",
      "Train Accuracy 91.65649165649165 | Test Accuracy 91.64124466137889\n",
      "Epoch 11 | Train loss 0.26716721057891846\n",
      "Train Accuracy 91.20879120879121 | Test Accuracy 91.58023184868821\n",
      "Epoch 12 | Train loss 0.2763855755329132\n",
      "Train Accuracy 91.9006919006919 | Test Accuracy 91.88529591214156\n",
      "Epoch 13 | Train loss 0.26654061675071716\n",
      "Train Accuracy 92.99959299959299 | Test Accuracy 93.16656497864551\n",
      "Epoch 14 | Train loss 0.24724026024341583\n",
      "Train Accuracy 93.12169312169311 | Test Accuracy 93.16656497864551\n",
      "Epoch 15 | Train loss 0.27397722005844116\n",
      "Train Accuracy 92.63329263329263 | Test Accuracy 92.61744966442953\n",
      "Epoch 16 | Train loss 0.23102276027202606\n",
      "Train Accuracy 93.61009361009361 | Test Accuracy 93.77669310555217\n",
      "Epoch 17 | Train loss 0.24679243564605713\n",
      "Train Accuracy 92.83679283679284 | Test Accuracy 92.37339841366688\n",
      "Epoch 18 | Train loss 0.2306451052427292\n",
      "Train Accuracy 93.81359381359381 | Test Accuracy 94.02074435631482\n",
      "Epoch 19 | Train loss 0.23911504447460175\n",
      "Train Accuracy 93.73219373219374 | Test Accuracy 94.14276998169616\n",
      "Epoch 20 | Train loss 0.24868056178092957\n",
      "Train Accuracy 93.81359381359381 | Test Accuracy 94.08175716900548\n",
      "Epoch 21 | Train loss 0.22696147859096527\n",
      "Train Accuracy 93.12169312169311 | Test Accuracy 93.41061622940818\n",
      "Epoch 22 | Train loss 0.2327035367488861\n",
      "Train Accuracy 94.26129426129425 | Test Accuracy 94.50884685784014\n",
      "Epoch 23 | Train loss 0.2239845097064972\n",
      "Train Accuracy 94.34269434269434 | Test Accuracy 94.99694935936547\n",
      "Epoch 24 | Train loss 0.2198677361011505\n",
      "Train Accuracy 94.5054945054945 | Test Accuracy 94.99694935936547\n",
      "Epoch 25 | Train loss 0.19004210829734802\n",
      "Train Accuracy 94.83109483109483 | Test Accuracy 94.32580841976815\n",
      "Epoch 26 | Train loss 0.20980024337768555\n",
      "Train Accuracy 94.7903947903948 | Test Accuracy 95.24100061012813\n",
      "Epoch 27 | Train loss 0.22752754390239716\n",
      "Train Accuracy 95.1159951159951 | Test Accuracy 95.36302623550945\n",
      "Epoch 28 | Train loss 0.2268570065498352\n",
      "Train Accuracy 94.74969474969474 | Test Accuracy 95.1189749847468\n",
      "Epoch 29 | Train loss 0.1829284280538559\n",
      "Train Accuracy 94.54619454619456 | Test Accuracy 95.05796217205614\n",
      "Epoch 30 | Train loss 0.22636328637599945\n",
      "Train Accuracy 95.07529507529507 | Test Accuracy 95.24100061012813\n",
      "Epoch 31 | Train loss 0.19493624567985535\n",
      "Train Accuracy 94.91249491249492 | Test Accuracy 95.36302623550945\n",
      "Epoch 32 | Train loss 0.21099761128425598\n",
      "Train Accuracy 95.52299552299553 | Test Accuracy 95.54606467358145\n",
      "Epoch 33 | Train loss 0.20574676990509033\n",
      "Train Accuracy 95.27879527879529 | Test Accuracy 95.66809029896278\n",
      "Epoch 34 | Train loss 0.21331486105918884\n",
      "Train Accuracy 95.31949531949532 | Test Accuracy 95.66809029896278\n",
      "Epoch 35 | Train loss 0.201002836227417\n",
      "Train Accuracy 95.64509564509565 | Test Accuracy 95.85112873703477\n",
      "Epoch 36 | Train loss 0.18223105370998383\n",
      "Train Accuracy 95.8078958078958 | Test Accuracy 95.66809029896278\n",
      "Epoch 37 | Train loss 0.2215328812599182\n",
      "Train Accuracy 96.01139601139602 | Test Accuracy 96.52226967663209\n",
      "Epoch 38 | Train loss 0.1978391855955124\n",
      "Train Accuracy 96.25559625559626 | Test Accuracy 96.3392312385601\n",
      "Epoch 39 | Train loss 0.19020214676856995\n",
      "Train Accuracy 95.68579568579568 | Test Accuracy 96.58328248932276\n",
      "Epoch 40 | Train loss 0.21475853025913239\n",
      "Train Accuracy 96.33699633699634 | Test Accuracy 96.76632092739476\n",
      "Epoch 41 | Train loss 0.21132779121398926\n",
      "Train Accuracy 96.33699633699634 | Test Accuracy 95.9731543624161\n",
      "Epoch 42 | Train loss 0.22040390968322754\n",
      "Train Accuracy 96.41839641839643 | Test Accuracy 96.09517998779744\n",
      "Epoch 43 | Train loss 0.182549849152565\n",
      "Train Accuracy 96.82539682539682 | Test Accuracy 97.07138499084807\n",
      "Epoch 44 | Train loss 0.1873258501291275\n",
      "Train Accuracy 96.98819698819699 | Test Accuracy 97.31543624161074\n",
      "Epoch 45 | Train loss 0.18129689991474152\n",
      "Train Accuracy 96.82539682539682 | Test Accuracy 97.1934106162294\n",
      "Epoch 46 | Train loss 0.1933000087738037\n",
      "Train Accuracy 96.98819698819699 | Test Accuracy 97.25442342892008\n",
      "Epoch 47 | Train loss 0.1715015321969986\n",
      "Train Accuracy 96.66259666259667 | Test Accuracy 96.09517998779744\n",
      "Epoch 48 | Train loss 0.156132772564888\n",
      "Train Accuracy 97.27309727309728 | Test Accuracy 97.3764490543014\n",
      "Epoch 49 | Train loss 0.17226605117321014\n",
      "Train Accuracy 96.90679690679691 | Test Accuracy 97.43746186699207\n",
      "Epoch 50 | Train loss 0.16269943118095398\n",
      "Train Accuracy 97.27309727309728 | Test Accuracy 97.62050030506406\n",
      "Epoch 51 | Train loss 0.1484001874923706\n",
      "Train Accuracy 97.55799755799755 | Test Accuracy 97.9255643685174\n",
      "Epoch 52 | Train loss 0.1702028214931488\n",
      "Train Accuracy 97.51729751729752 | Test Accuracy 97.68151311775472\n",
      "Epoch 53 | Train loss 0.1789754033088684\n",
      "Train Accuracy 97.5986975986976 | Test Accuracy 97.80353874313606\n",
      "Epoch 54 | Train loss 0.14124009013175964\n",
      "Train Accuracy 97.55799755799755 | Test Accuracy 97.3764490543014\n",
      "Epoch 55 | Train loss 0.1678040623664856\n",
      "Train Accuracy 97.84289784289784 | Test Accuracy 98.10860280658939\n",
      "Epoch 56 | Train loss 0.1638336330652237\n",
      "Train Accuracy 97.55799755799755 | Test Accuracy 97.3764490543014\n",
      "Epoch 57 | Train loss 0.18044163286685944\n",
      "Train Accuracy 98.04639804639804 | Test Accuracy 98.47467968273338\n",
      "Epoch 58 | Train loss 0.16597561538219452\n",
      "Train Accuracy 97.55799755799755 | Test Accuracy 97.9255643685174\n",
      "Epoch 59 | Train loss 0.1517064869403839\n",
      "Train Accuracy 97.72079772079772 | Test Accuracy 97.55948749237339\n",
      "Epoch 60 | Train loss 0.12597231566905975\n",
      "Train Accuracy 98.2091982091982 | Test Accuracy 98.53569249542404\n",
      "Epoch 61 | Train loss 0.14778386056423187\n",
      "Train Accuracy 97.84289784289784 | Test Accuracy 97.68151311775472\n",
      "Epoch 62 | Train loss 0.1538880467414856\n",
      "Train Accuracy 98.4126984126984 | Test Accuracy 98.53569249542404\n",
      "Epoch 63 | Train loss 0.143718421459198\n",
      "Train Accuracy 98.53479853479854 | Test Accuracy 98.59670530811471\n",
      "Epoch 64 | Train loss 0.13015547394752502\n",
      "Train Accuracy 98.24989824989825 | Test Accuracy 98.16961561928005\n",
      "Epoch 65 | Train loss 0.15673451125621796\n",
      "Train Accuracy 98.08709808709808 | Test Accuracy 98.04758999389873\n",
      "Epoch 66 | Train loss 0.16302429139614105\n",
      "Train Accuracy 98.4940984940985 | Test Accuracy 98.47467968273338\n",
      "Epoch 67 | Train loss 0.14063243567943573\n",
      "Train Accuracy 97.68009768009767 | Test Accuracy 97.80353874313606\n",
      "Epoch 68 | Train loss 0.1407904475927353\n",
      "Train Accuracy 98.53479853479854 | Test Accuracy 98.29164124466138\n",
      "Epoch 69 | Train loss 0.14093361794948578\n",
      "Train Accuracy 98.12779812779813 | Test Accuracy 97.9255643685174\n",
      "Epoch 70 | Train loss 0.15656040608882904\n",
      "Train Accuracy 98.86039886039886 | Test Accuracy 98.65771812080537\n",
      "Epoch 71 | Train loss 0.12166029214859009\n",
      "Train Accuracy 97.5986975986976 | Test Accuracy 97.49847467968273\n",
      "Epoch 72 | Train loss 0.1356125921010971\n",
      "Train Accuracy 98.73829873829874 | Test Accuracy 98.53569249542404\n",
      "Epoch 73 | Train loss 0.10960333049297333\n",
      "Train Accuracy 97.84289784289784 | Test Accuracy 97.62050030506406\n",
      "Epoch 74 | Train loss 0.13629619777202606\n",
      "Train Accuracy 98.98249898249898 | Test Accuracy 98.53569249542404\n",
      "Epoch 75 | Train loss 0.11808023601770401\n",
      "Train Accuracy 98.81969881969881 | Test Accuracy 98.65771812080537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 | Train loss 0.12972289323806763\n",
      "Train Accuracy 98.94179894179894 | Test Accuracy 98.90176937156802\n",
      "Epoch 77 | Train loss 0.14269527792930603\n",
      "Train Accuracy 98.77899877899878 | Test Accuracy 98.65771812080537\n",
      "Epoch 78 | Train loss 0.11841492354869843\n",
      "Train Accuracy 98.81969881969881 | Test Accuracy 98.7797437461867\n",
      "Epoch 79 | Train loss 0.12571583688259125\n",
      "Train Accuracy 98.86039886039886 | Test Accuracy 98.7797437461867\n",
      "Epoch 80 | Train loss 0.12559713423252106\n",
      "Train Accuracy 98.9010989010989 | Test Accuracy 98.96278218425869\n",
      "Epoch 81 | Train loss 0.10437126457691193\n",
      "Train Accuracy 98.65689865689866 | Test Accuracy 98.53569249542404\n",
      "Epoch 82 | Train loss 0.12889762222766876\n",
      "Train Accuracy 98.9010989010989 | Test Accuracy 98.7797437461867\n",
      "Epoch 83 | Train loss 0.10427843779325485\n",
      "Train Accuracy 99.1045991045991 | Test Accuracy 99.02379499694935\n",
      "Epoch 84 | Train loss 0.0913592204451561\n",
      "Train Accuracy 98.61619861619862 | Test Accuracy 98.53569249542404\n",
      "Epoch 85 | Train loss 0.1154637560248375\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.14582062233069\n",
      "Epoch 86 | Train loss 0.11197886615991592\n",
      "Train Accuracy 99.18599918599918 | Test Accuracy 99.14582062233069\n",
      "Epoch 87 | Train loss 0.11453887820243835\n",
      "Train Accuracy 98.98249898249898 | Test Accuracy 98.84075655887736\n",
      "Epoch 88 | Train loss 0.12582039833068848\n",
      "Train Accuracy 98.86039886039886 | Test Accuracy 98.71873093349603\n",
      "Epoch 89 | Train loss 0.10290847718715668\n",
      "Train Accuracy 99.26739926739927 | Test Accuracy 99.14582062233069\n",
      "Epoch 90 | Train loss 0.10566970705986023\n",
      "Train Accuracy 99.26739926739927 | Test Accuracy 98.96278218425869\n",
      "Epoch 91 | Train loss 0.10619413107633591\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.14582062233069\n",
      "Epoch 92 | Train loss 0.11069779098033905\n",
      "Train Accuracy 98.94179894179894 | Test Accuracy 98.90176937156802\n",
      "Epoch 93 | Train loss 0.11566852778196335\n",
      "Train Accuracy 99.26739926739927 | Test Accuracy 98.96278218425869\n",
      "Epoch 94 | Train loss 0.1131133884191513\n",
      "Train Accuracy 99.02319902319903 | Test Accuracy 98.90176937156802\n",
      "Epoch 95 | Train loss 0.12378252297639847\n",
      "Train Accuracy 99.34879934879935 | Test Accuracy 99.14582062233069\n",
      "Epoch 96 | Train loss 0.12298669666051865\n",
      "Train Accuracy 99.34879934879935 | Test Accuracy 99.20683343502135\n",
      "Epoch 97 | Train loss 0.10704959183931351\n",
      "Train Accuracy 99.26739926739927 | Test Accuracy 99.02379499694935\n",
      "Epoch 98 | Train loss 0.1137673482298851\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.20683343502135\n",
      "Epoch 99 | Train loss 0.09862245619297028\n",
      "Train Accuracy 98.94179894179894 | Test Accuracy 98.90176937156802\n",
      "Epoch 100 | Train loss 0.0883193090558052\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 98.84075655887736\n",
      "Epoch 101 | Train loss 0.10809893161058426\n",
      "Train Accuracy 98.94179894179894 | Test Accuracy 98.90176937156802\n",
      "Epoch 102 | Train loss 0.09109966456890106\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 98.96278218425869\n",
      "Epoch 103 | Train loss 0.1005614772439003\n",
      "Train Accuracy 98.94179894179894 | Test Accuracy 98.90176937156802\n",
      "Epoch 104 | Train loss 0.09784556180238724\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.02379499694935\n",
      "Epoch 105 | Train loss 0.09313280880451202\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.20683343502135\n",
      "Epoch 106 | Train loss 0.08936735987663269\n",
      "Train Accuracy 99.18599918599918 | Test Accuracy 98.90176937156802\n",
      "Epoch 107 | Train loss 0.10349653661251068\n",
      "Train Accuracy 99.06389906389906 | Test Accuracy 98.96278218425869\n",
      "Epoch 108 | Train loss 0.07491891831159592\n",
      "Train Accuracy 99.38949938949939 | Test Accuracy 99.02379499694935\n",
      "Epoch 109 | Train loss 0.08508113771677017\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.02379499694935\n",
      "Epoch 110 | Train loss 0.1119031235575676\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.20683343502135\n",
      "Epoch 111 | Train loss 0.08676667511463165\n",
      "Train Accuracy 98.77899877899878 | Test Accuracy 98.7797437461867\n",
      "Epoch 112 | Train loss 0.07577633112668991\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.02379499694935\n",
      "Epoch 113 | Train loss 0.07970450073480606\n",
      "Train Accuracy 99.38949938949939 | Test Accuracy 99.02379499694935\n",
      "Epoch 114 | Train loss 0.09343411028385162\n",
      "Train Accuracy 99.38949938949939 | Test Accuracy 99.08480780964003\n",
      "Epoch 115 | Train loss 0.12849658727645874\n",
      "Train Accuracy 99.26739926739927 | Test Accuracy 99.02379499694935\n",
      "Epoch 116 | Train loss 0.08486147969961166\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 98.96278218425869\n",
      "Epoch 117 | Train loss 0.07714910805225372\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.02379499694935\n",
      "Epoch 118 | Train loss 0.08343805372714996\n",
      "Train Accuracy 98.65689865689866 | Test Accuracy 98.71873093349603\n",
      "Epoch 119 | Train loss 0.09859874099493027\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.20683343502135\n",
      "Epoch 120 | Train loss 0.07904688268899918\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.20683343502135\n",
      "Epoch 121 | Train loss 0.09087850898504257\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.20683343502135\n",
      "Epoch 122 | Train loss 0.08042100816965103\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.08480780964003\n",
      "Epoch 123 | Train loss 0.10335252434015274\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.14582062233069\n",
      "Epoch 124 | Train loss 0.0747457966208458\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.20683343502135\n",
      "Epoch 125 | Train loss 0.0989147275686264\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.26784624771201\n",
      "Epoch 126 | Train loss 0.07686208933591843\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.14582062233069\n",
      "Epoch 127 | Train loss 0.0721205547451973\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 128 | Train loss 0.06380676478147507\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.14582062233069\n",
      "Epoch 129 | Train loss 0.06922703236341476\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.02379499694935\n",
      "Epoch 130 | Train loss 0.07461253553628922\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.20683343502135\n",
      "Epoch 131 | Train loss 0.0697217732667923\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.20683343502135\n",
      "Epoch 132 | Train loss 0.07207470387220383\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.26784624771201\n",
      "Epoch 133 | Train loss 0.07059505581855774\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 134 | Train loss 0.06972644478082657\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.26784624771201\n",
      "Epoch 135 | Train loss 0.06640816479921341\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 136 | Train loss 0.08139169216156006\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.26784624771201\n",
      "Epoch 137 | Train loss 0.06469430774450302\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.14582062233069\n",
      "Epoch 138 | Train loss 0.07149716466665268\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.26784624771201\n",
      "Epoch 139 | Train loss 0.07812051475048065\n",
      "Train Accuracy 99.14529914529915 | Test Accuracy 98.90176937156802\n",
      "Epoch 140 | Train loss 0.06595698744058609\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.14582062233069\n",
      "Epoch 141 | Train loss 0.06470441073179245\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.26784624771201\n",
      "Epoch 142 | Train loss 0.08492784202098846\n",
      "Train Accuracy 99.34879934879935 | Test Accuracy 99.08480780964003\n",
      "Epoch 143 | Train loss 0.06698209792375565\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.32885906040269\n",
      "Epoch 144 | Train loss 0.06964259594678879\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 145 | Train loss 0.06977657973766327\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 146 | Train loss 0.05961397662758827\n",
      "Train Accuracy 99.18599918599918 | Test Accuracy 98.96278218425869\n",
      "Epoch 147 | Train loss 0.0623420774936676\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 148 | Train loss 0.07127519696950912\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.26784624771201\n",
      "Epoch 149 | Train loss 0.05090588703751564\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.20683343502135\n",
      "Epoch 150 | Train loss 0.06775767356157303\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.14582062233069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151 | Train loss 0.07229060679674149\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.20683343502135\n",
      "Epoch 152 | Train loss 0.06120862439274788\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.20683343502135\n",
      "Epoch 153 | Train loss 0.06579317152500153\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.32885906040269\n",
      "Epoch 154 | Train loss 0.05331505089998245\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.26784624771201\n",
      "Epoch 155 | Train loss 0.06411831080913544\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.32885906040269\n",
      "Epoch 156 | Train loss 0.060359612107276917\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.26784624771201\n",
      "Epoch 157 | Train loss 0.07096514105796814\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.02379499694935\n",
      "Epoch 158 | Train loss 0.052890025079250336\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 159 | Train loss 0.0666571706533432\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.32885906040269\n",
      "Epoch 160 | Train loss 0.0507986955344677\n",
      "Train Accuracy 98.94179894179894 | Test Accuracy 98.90176937156802\n",
      "Epoch 161 | Train loss 0.07146592438220978\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 162 | Train loss 0.06579351425170898\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.32885906040269\n",
      "Epoch 163 | Train loss 0.05871296674013138\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.32885906040269\n",
      "Epoch 164 | Train loss 0.06300711631774902\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.38987187309336\n",
      "Epoch 165 | Train loss 0.0668310895562172\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.38987187309336\n",
      "Epoch 166 | Train loss 0.047862179577350616\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.20683343502135\n",
      "Epoch 167 | Train loss 0.035659607499837875\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.32885906040269\n",
      "Epoch 168 | Train loss 0.052420809864997864\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.20683343502135\n",
      "Epoch 169 | Train loss 0.05287255346775055\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.38987187309336\n",
      "Epoch 170 | Train loss 0.05564387887716293\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.20683343502135\n",
      "Epoch 171 | Train loss 0.05682936683297157\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.32885906040269\n",
      "Epoch 172 | Train loss 0.04445003718137741\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.38987187309336\n",
      "Epoch 173 | Train loss 0.042628977447748184\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.38987187309336\n",
      "Epoch 174 | Train loss 0.04966561868786812\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.38987187309336\n",
      "Epoch 175 | Train loss 0.06394096463918686\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 176 | Train loss 0.05356110632419586\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.20683343502135\n",
      "Epoch 177 | Train loss 0.04593716934323311\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.38987187309336\n",
      "Epoch 178 | Train loss 0.036526717245578766\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.20683343502135\n",
      "Epoch 179 | Train loss 0.04525437206029892\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 180 | Train loss 0.04775878041982651\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.32885906040269\n",
      "Epoch 181 | Train loss 0.053196851164102554\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.32885906040269\n",
      "Epoch 182 | Train loss 0.054744042456150055\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 183 | Train loss 0.06598752737045288\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 184 | Train loss 0.055851057171821594\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 185 | Train loss 0.04905010759830475\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.26784624771201\n",
      "Epoch 186 | Train loss 0.042170777916908264\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.32885906040269\n",
      "Epoch 187 | Train loss 0.045195553451776505\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.32885906040269\n",
      "Epoch 188 | Train loss 0.05341614782810211\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 189 | Train loss 0.044860582798719406\n",
      "Train Accuracy 99.3080993080993 | Test Accuracy 99.02379499694935\n",
      "Epoch 190 | Train loss 0.044774048030376434\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.32885906040269\n",
      "Epoch 191 | Train loss 0.0479043573141098\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.38987187309336\n",
      "Epoch 192 | Train loss 0.04091647267341614\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.32885906040269\n",
      "Epoch 193 | Train loss 0.041915275156497955\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.45088468578402\n",
      "Epoch 194 | Train loss 0.04004081338644028\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.45088468578402\n",
      "Epoch 195 | Train loss 0.02784603089094162\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.51189749847468\n",
      "Epoch 196 | Train loss 0.05637194961309433\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.51189749847468\n",
      "Epoch 197 | Train loss 0.03665688633918762\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 198 | Train loss 0.03202259540557861\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.45088468578402\n",
      "Epoch 199 | Train loss 0.04047228768467903\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.14582062233069\n",
      "Epoch 200 | Train loss 0.04944787919521332\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.32885906040269\n",
      "Epoch 201 | Train loss 0.05520249158143997\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.32885906040269\n",
      "Epoch 202 | Train loss 0.04892165586352348\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.51189749847468\n",
      "Epoch 203 | Train loss 0.037809573113918304\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.51189749847468\n",
      "Epoch 204 | Train loss 0.0369512215256691\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.08480780964003\n",
      "Epoch 205 | Train loss 0.04175464063882828\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 206 | Train loss 0.03717032074928284\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 207 | Train loss 0.03402036055922508\n",
      "Train Accuracy 99.3080993080993 | Test Accuracy 99.02379499694935\n",
      "Epoch 208 | Train loss 0.029802784323692322\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 209 | Train loss 0.03838571533560753\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.38987187309336\n",
      "Epoch 210 | Train loss 0.044117458164691925\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.26784624771201\n",
      "Epoch 211 | Train loss 0.035118285566568375\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 212 | Train loss 0.04855581000447273\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.32885906040269\n",
      "Epoch 213 | Train loss 0.03645486757159233\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 214 | Train loss 0.033338483422994614\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.32885906040269\n",
      "Epoch 215 | Train loss 0.033194649964571\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.38987187309336\n",
      "Epoch 216 | Train loss 0.03399502858519554\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.51189749847468\n",
      "Epoch 217 | Train loss 0.04083457589149475\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.32885906040269\n",
      "Epoch 218 | Train loss 0.04647156596183777\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 219 | Train loss 0.03559567406773567\n",
      "Train Accuracy 99.14529914529915 | Test Accuracy 99.02379499694935\n",
      "Epoch 220 | Train loss 0.02767913229763508\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.38987187309336\n",
      "Epoch 221 | Train loss 0.053435806185007095\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 222 | Train loss 0.03361276537179947\n",
      "Train Accuracy 99.47089947089947 | Test Accuracy 99.26784624771201\n",
      "Epoch 223 | Train loss 0.031049732118844986\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.51189749847468\n",
      "Epoch 224 | Train loss 0.06556746363639832\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 225 | Train loss 0.04342060908675194\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.08480780964003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226 | Train loss 0.032711323350667953\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.32885906040269\n",
      "Epoch 227 | Train loss 0.02888377755880356\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 228 | Train loss 0.03092672862112522\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.45088468578402\n",
      "Epoch 229 | Train loss 0.043851338326931\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.38987187309336\n",
      "Epoch 230 | Train loss 0.03599230572581291\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 231 | Train loss 0.024900326505303383\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 232 | Train loss 0.026323162019252777\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 233 | Train loss 0.03634965047240257\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 234 | Train loss 0.029081933200359344\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 235 | Train loss 0.034290093928575516\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 236 | Train loss 0.02553032897412777\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.45088468578402\n",
      "Epoch 237 | Train loss 0.023034948855638504\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.45088468578402\n",
      "Epoch 238 | Train loss 0.024284400045871735\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 239 | Train loss 0.034068748354911804\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 240 | Train loss 0.025863032788038254\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.45088468578402\n",
      "Epoch 241 | Train loss 0.03543401136994362\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 242 | Train loss 0.028427470475435257\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 243 | Train loss 0.03196045756340027\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 244 | Train loss 0.025266528129577637\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 245 | Train loss 0.040546976029872894\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 246 | Train loss 0.02954276092350483\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.32885906040269\n",
      "Epoch 247 | Train loss 0.019410772249102592\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 248 | Train loss 0.0289139524102211\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 249 | Train loss 0.02632388286292553\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.14582062233069\n",
      "Epoch 250 | Train loss 0.0322098582983017\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 251 | Train loss 0.02391468547284603\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 252 | Train loss 0.025234121829271317\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 253 | Train loss 0.03708088397979736\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 254 | Train loss 0.038685593754053116\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 255 | Train loss 0.02913656458258629\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 256 | Train loss 0.038093723356723785\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 257 | Train loss 0.04425091668963432\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 258 | Train loss 0.02460777573287487\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 259 | Train loss 0.02848990075290203\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 260 | Train loss 0.02814554050564766\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 261 | Train loss 0.025224756449460983\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 262 | Train loss 0.04772759974002838\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 263 | Train loss 0.024291057139635086\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 264 | Train loss 0.023133542388677597\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 265 | Train loss 0.027239933609962463\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 266 | Train loss 0.018556389957666397\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 267 | Train loss 0.032300252467393875\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 268 | Train loss 0.03202067315578461\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 269 | Train loss 0.02858916111290455\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 270 | Train loss 0.0370153933763504\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 271 | Train loss 0.021354397758841515\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 272 | Train loss 0.028155632317066193\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 273 | Train loss 0.02331269532442093\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 274 | Train loss 0.027934584766626358\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 275 | Train loss 0.03413345664739609\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 276 | Train loss 0.032766714692115784\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 277 | Train loss 0.05995279178023338\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 278 | Train loss 0.03233925998210907\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 279 | Train loss 0.016168689355254173\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.38987187309336\n",
      "Epoch 280 | Train loss 0.03810032829642296\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 281 | Train loss 0.04437188804149628\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 282 | Train loss 0.0261481162160635\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 283 | Train loss 0.04638572409749031\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 284 | Train loss 0.025097236037254333\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 285 | Train loss 0.027498815208673477\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 286 | Train loss 0.020240429788827896\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.38987187309336\n",
      "Epoch 287 | Train loss 0.042429760098457336\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 288 | Train loss 0.03173194080591202\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 289 | Train loss 0.034911368042230606\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 290 | Train loss 0.0237413477152586\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 291 | Train loss 0.021692238748073578\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 292 | Train loss 0.024586759507656097\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 293 | Train loss 0.013866067864000797\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 294 | Train loss 0.024250054731965065\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 295 | Train loss 0.020398732274770737\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 296 | Train loss 0.022001877427101135\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 297 | Train loss 0.0304130669683218\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 298 | Train loss 0.02671143226325512\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 299 | Train loss 0.024915490299463272\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 300 | Train loss 0.019903650507330894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 301 | Train loss 0.031475506722927094\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 302 | Train loss 0.024512210860848427\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 303 | Train loss 0.014314848929643631\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 304 | Train loss 0.025660421699285507\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.57291031116534\n",
      "Epoch 305 | Train loss 0.025018155574798584\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 306 | Train loss 0.02141355350613594\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 307 | Train loss 0.022104019299149513\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 308 | Train loss 0.013795880600810051\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 309 | Train loss 0.024647759273648262\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 310 | Train loss 0.03362707793712616\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 311 | Train loss 0.03321589156985283\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.57291031116534\n",
      "Epoch 312 | Train loss 0.015219230204820633\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.38987187309336\n",
      "Epoch 313 | Train loss 0.021064715459942818\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 314 | Train loss 0.035783782601356506\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 315 | Train loss 0.020571717992424965\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 316 | Train loss 0.01877221092581749\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 317 | Train loss 0.019256360828876495\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 318 | Train loss 0.02363772504031658\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 319 | Train loss 0.026924461126327515\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 320 | Train loss 0.020528223365545273\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 321 | Train loss 0.01846105232834816\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 322 | Train loss 0.016827762126922607\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 323 | Train loss 0.01876978389918804\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 324 | Train loss 0.02745852805674076\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.57291031116534\n",
      "Epoch 325 | Train loss 0.02112591825425625\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 326 | Train loss 0.019215798005461693\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.57291031116534\n",
      "Epoch 327 | Train loss 0.026394756510853767\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 328 | Train loss 0.03474976867437363\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 329 | Train loss 0.033660564571619034\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 330 | Train loss 0.02985263615846634\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 331 | Train loss 0.033618196845054626\n",
      "Train Accuracy 99.63369963369964 | Test Accuracy 99.45088468578402\n",
      "Epoch 332 | Train loss 0.016903106123209\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 333 | Train loss 0.021476728841662407\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 334 | Train loss 0.028021330013871193\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 335 | Train loss 0.021207096055150032\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.38987187309336\n",
      "Epoch 336 | Train loss 0.021652687340974808\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 337 | Train loss 0.021939702332019806\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.57291031116534\n",
      "Epoch 338 | Train loss 0.021675314754247665\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 339 | Train loss 0.020893489941954613\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 340 | Train loss 0.02290712669491768\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 341 | Train loss 0.01136623416095972\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 342 | Train loss 0.013179780915379524\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 343 | Train loss 0.019490700215101242\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 344 | Train loss 0.013623583130538464\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 345 | Train loss 0.017789972946047783\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 346 | Train loss 0.01907024346292019\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 347 | Train loss 0.01457761600613594\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 348 | Train loss 0.018620328977704048\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 349 | Train loss 0.018551386892795563\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 350 | Train loss 0.017654187977313995\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 351 | Train loss 0.030416104942560196\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.38987187309336\n",
      "Epoch 352 | Train loss 0.017441023141145706\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 353 | Train loss 0.04929424077272415\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 354 | Train loss 0.022776756435632706\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 355 | Train loss 0.01881289668381214\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 356 | Train loss 0.023536765947937965\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 357 | Train loss 0.017303068190813065\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 358 | Train loss 0.011431751772761345\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 359 | Train loss 0.015326590277254581\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 360 | Train loss 0.01838938146829605\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 361 | Train loss 0.017696935683488846\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 362 | Train loss 0.01280080247670412\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 363 | Train loss 0.017015032470226288\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 364 | Train loss 0.020044229924678802\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 365 | Train loss 0.025134924799203873\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 366 | Train loss 0.016821514815092087\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 367 | Train loss 0.03660309314727783\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 368 | Train loss 0.031786318868398666\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 369 | Train loss 0.03730574995279312\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 370 | Train loss 0.01622314378619194\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 371 | Train loss 0.0301631111651659\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 372 | Train loss 0.011326824314892292\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 373 | Train loss 0.036270514130592346\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 374 | Train loss 0.029307732358574867\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.08480780964003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375 | Train loss 0.01557453814893961\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.08480780964003\n",
      "Epoch 376 | Train loss 0.020806021988391876\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 377 | Train loss 0.02331693470478058\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 378 | Train loss 0.02701166830956936\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 379 | Train loss 0.02102089114487171\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 380 | Train loss 0.011618003249168396\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 381 | Train loss 0.016607100144028664\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 382 | Train loss 0.010777733288705349\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 383 | Train loss 0.018506767228245735\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 384 | Train loss 0.011853511445224285\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 385 | Train loss 0.026852969080209732\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.51189749847468\n",
      "Epoch 386 | Train loss 0.018604421988129616\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 387 | Train loss 0.011110952123999596\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 388 | Train loss 0.010938690975308418\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 389 | Train loss 0.017300141975283623\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 390 | Train loss 0.02421337552368641\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 391 | Train loss 0.017665427178144455\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 392 | Train loss 0.019443059340119362\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 393 | Train loss 0.013056537136435509\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 394 | Train loss 0.011546846479177475\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 395 | Train loss 0.011613375507295132\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 396 | Train loss 0.03489389270544052\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 397 | Train loss 0.01749638095498085\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 398 | Train loss 0.028715327382087708\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 399 | Train loss 0.02689710259437561\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 400 | Train loss 0.022507868707180023\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 401 | Train loss 0.017615653574466705\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 402 | Train loss 0.01637709140777588\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.45088468578402\n",
      "Epoch 403 | Train loss 0.01266427244991064\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 404 | Train loss 0.018043098971247673\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 405 | Train loss 0.009992067702114582\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 406 | Train loss 0.017490915954113007\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 407 | Train loss 0.021706299856305122\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 408 | Train loss 0.011843730695545673\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.45088468578402\n",
      "Epoch 409 | Train loss 0.013150444254279137\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 410 | Train loss 0.011853756383061409\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 411 | Train loss 0.012942301109433174\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 412 | Train loss 0.017792832106351852\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 413 | Train loss 0.007371921092271805\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 414 | Train loss 0.012283318676054478\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 415 | Train loss 0.012708701193332672\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 416 | Train loss 0.02511010877788067\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 417 | Train loss 0.03360769897699356\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 418 | Train loss 0.019975809380412102\n",
      "Train Accuracy 99.43019943019942 | Test Accuracy 99.20683343502135\n",
      "Epoch 419 | Train loss 0.011711622588336468\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 420 | Train loss 0.025357449427247047\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 421 | Train loss 0.015929216518998146\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 422 | Train loss 0.018733225762844086\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 423 | Train loss 0.0076383063569664955\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 424 | Train loss 0.01539175771176815\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 425 | Train loss 0.017874019220471382\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 426 | Train loss 0.012956243939697742\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.63392312385601\n",
      "Epoch 427 | Train loss 0.021340742707252502\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 428 | Train loss 0.022387297824025154\n",
      "Train Accuracy 99.75579975579976 | Test Accuracy 99.51189749847468\n",
      "Epoch 429 | Train loss 0.025353701785206795\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 430 | Train loss 0.021933848038315773\n",
      "Train Accuracy 99.67439967439967 | Test Accuracy 99.38987187309336\n",
      "Epoch 431 | Train loss 0.028602803125977516\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.51189749847468\n",
      "Epoch 432 | Train loss 0.011514201760292053\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 433 | Train loss 0.007790200877934694\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 434 | Train loss 0.017269372940063477\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 435 | Train loss 0.011369196698069572\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 436 | Train loss 0.009654385969042778\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 437 | Train loss 0.012811677530407906\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 438 | Train loss 0.020838722586631775\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 439 | Train loss 0.010792193934321404\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 440 | Train loss 0.015541143715381622\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 441 | Train loss 0.011915410868823528\n",
      "Train Accuracy 99.55229955229954 | Test Accuracy 99.38987187309336\n",
      "Epoch 442 | Train loss 0.02405460923910141\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.45088468578402\n",
      "Epoch 443 | Train loss 0.020700545981526375\n",
      "Train Accuracy 99.51159951159951 | Test Accuracy 99.26784624771201\n",
      "Epoch 444 | Train loss 0.013074020855128765\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.63392312385601\n",
      "Epoch 445 | Train loss 0.007151004392653704\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 446 | Train loss 0.011103895492851734\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 447 | Train loss 0.017625974491238594\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 448 | Train loss 0.011308840475976467\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 | Train loss 0.012812490575015545\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 450 | Train loss 0.016105523332953453\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 451 | Train loss 0.010558735579252243\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 452 | Train loss 0.027034837752580643\n",
      "Train Accuracy 99.22669922669922 | Test Accuracy 99.14582062233069\n",
      "Epoch 453 | Train loss 0.022710712626576424\n",
      "Train Accuracy 99.87789987789988 | Test Accuracy 99.45088468578402\n",
      "Epoch 454 | Train loss 0.012491337954998016\n",
      "Train Accuracy 99.87789987789988 | Test Accuracy 99.45088468578402\n",
      "Epoch 455 | Train loss 0.008951723575592041\n",
      "Train Accuracy 99.3080993080993 | Test Accuracy 99.14582062233069\n",
      "Epoch 456 | Train loss 0.009639565832912922\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.51189749847468\n",
      "Epoch 457 | Train loss 0.010153346695005894\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 458 | Train loss 0.025614267215132713\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 459 | Train loss 0.011706636287271976\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.57291031116534\n",
      "Epoch 460 | Train loss 0.012682626023888588\n",
      "Train Accuracy 99.18599918599918 | Test Accuracy 98.96278218425869\n",
      "Epoch 461 | Train loss 0.025033622980117798\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 462 | Train loss 0.01230095885694027\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 463 | Train loss 0.018326018005609512\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.63392312385601\n",
      "Epoch 464 | Train loss 0.009606762789189816\n",
      "Train Accuracy 99.5929995929996 | Test Accuracy 99.38987187309336\n",
      "Epoch 465 | Train loss 0.011009075678884983\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 466 | Train loss 0.023222165182232857\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.57291031116534\n",
      "Epoch 467 | Train loss 0.010377776809036732\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 468 | Train loss 0.005747190210968256\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 469 | Train loss 0.0081044752150774\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 470 | Train loss 0.01457966584712267\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 471 | Train loss 0.011173699982464314\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 472 | Train loss 0.012702318839728832\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.51189749847468\n",
      "Epoch 473 | Train loss 0.009419167414307594\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.63392312385601\n",
      "Epoch 474 | Train loss 0.009548173286020756\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n",
      "Epoch 475 | Train loss 0.007478665094822645\n",
      "Train Accuracy 99.71509971509973 | Test Accuracy 99.45088468578402\n",
      "Epoch 476 | Train loss 0.014633686281740665\n",
      "Train Accuracy 99.87789987789988 | Test Accuracy 99.51189749847468\n",
      "Epoch 477 | Train loss 0.012384836561977863\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.63392312385601\n",
      "Epoch 478 | Train loss 0.013646137900650501\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.63392312385601\n",
      "Epoch 479 | Train loss 0.01361903827637434\n",
      "Train Accuracy 99.83719983719985 | Test Accuracy 99.63392312385601\n",
      "Epoch 480 | Train loss 0.013655968010425568\n",
      "Train Accuracy 99.79649979649979 | Test Accuracy 99.57291031116534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m bestModel, highestAcc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     loss, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, features_to_use)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     pred, embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_to_use\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(pred)\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39mfloat(), batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Fall 2022/Independent Work/thesis/train-2d/gin/gin.py:45\u001b[0m, in \u001b[0;36mGIN.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index)\n\u001b[1;32m     44\u001b[0m h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h1, edge_index)\n\u001b[0;32m---> 45\u001b[0m h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m h4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(h3, edge_index)\n\u001b[1;32m     47\u001b[0m h5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(h4, edge_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/gin_conv.py:80\u001b[0m, in \u001b[0;36mGINConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     77\u001b[0m     x: OptPairTensor \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:484\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    487\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:608\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Tensor, index: Tensor,\n\u001b[1;32m    596\u001b[0m               ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m               dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:109\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/aggr/basic.py:21\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/aggr/base.py:155\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m segment(x, ptr, reduce\u001b[38;5;241m=\u001b[39mreduce)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     77\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "bestModel, highestAcc = None, 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss, model = train(model, train_loader, [0])\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model, train_loader, [0]), check_accuracy(model, test_loader, [0])\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "#     scheduler.step(test_acc)\n",
    "    \n",
    "    if test_acc > highestAcc:\n",
    "        highestAcc = test_acc\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square\n",
    "import networkx as nx\n",
    "square = nx.Graph()\n",
    "square.add_edge(0, 1)\n",
    "square.add_edge(1, 3)\n",
    "square.add_edge(0, 2)\n",
    "square.add_edge(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square with cross bar (rigid)\n",
    "import networkx as nx\n",
    "square_bar = nx.Graph()\n",
    "square_bar.add_edge(0, 1)\n",
    "square_bar.add_edge(1, 3)\n",
    "square_bar.add_edge(0, 2)\n",
    "square_bar.add_edge(2, 3)\n",
    "square_bar.add_edge(0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangle\n",
    "import networkx as nx\n",
    "triangle = nx.Graph()\n",
    "triangle.add_edge(0, 1)\n",
    "triangle.add_edge(0, 2)\n",
    "triangle.add_edge(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentagon\n",
    "import networkx as nx\n",
    "pentagon = nx.Graph()\n",
    "pentagon.add_edge(0, 1)\n",
    "pentagon.add_edge(1, 3)\n",
    "pentagon.add_edge(3, 4)\n",
    "pentagon.add_edge(4, 2)\n",
    "pentagon.add_edge(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8556]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[1.]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "toy_problems = [square, square_bar, triangle, pentagon]\n",
    "labels = [1, 0, 0, 1]\n",
    "\n",
    "for index, toy_problem in enumerate(toy_problems):\n",
    "    graph_as_data = from_networkx(toy_problem)\n",
    "    graph_as_data.x = generate_feature_vector(toy_problem)\n",
    "    graph_as_data.label = labels[index]\n",
    "    validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)\n",
    "    for batch in validation_set:\n",
    "        pred = bestModel(batch.x[:, [0]], batch.edge_index, batch.batch)\n",
    "        print(pred[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 10], num_nodes=5, x=[5, 2], label=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_as_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[41.9681]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for batch in validation_set:\n",
    "    pred = bestModel(batch.x, batch.edge_index, batch.batch)\n",
    "    print(pred[0])\n",
    "    print(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanTestDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_test.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        # processing code here\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        ind = 0\n",
    "        # convert from graph to Data object\n",
    "        for graph in total_laman_data[0]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 1)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        ind = 0\n",
    "        for graph in total_laman_data[1]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 64)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add functionality to support a test dataset\n",
    "TEST_DATA_PATH = \"../data-2d/data/test-dataset-30loc-5std.pkl.gz\"\n",
    "laman_test_set = LamanTestDataset(\"\", TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "laman_test_loader = DataLoader(laman_test_set, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 96.875\n"
     ]
    }
   ],
   "source": [
    "random_test_acc = check_accuracy(bestModel, laman_test_loader)\n",
    "print(f\"Accuracy {random_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate statistics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_coefficient(square, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_coefficient(triangle, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustering_coefficient(square_bar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate graph correlating clustering coefficient to rigidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'networkx.classes.digraph.DiGraph'>\n"
     ]
    }
   ],
   "source": [
    "for item in train_data:\n",
    "    item = to_networkx(item)\n",
    "    print(type(item))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.utils.convert.to_networkx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work: Sahil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_laman_data = None\n",
    "with gzip.open(DATA_PATH, 'r') as f:\n",
    "    total_laman_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graph = total_laman_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'networkx.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sample_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_data = from_networkx(sample_graph)\n",
    "from_data = to_networkx(to_data, to_undirected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'networkx.classes.graph.Graph'>\n"
     ]
    }
   ],
   "source": [
    "print(type(from_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_clustering_coefficient(G):\n",
    "    min_coefficient = 1\n",
    "    for node in G.nodes():\n",
    "        min_coefficient = min(min_coefficient, clustering_coefficient(G, node))\n",
    "        \n",
    "    return min_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])   0   0.0\n",
      "tensor([1])   1   0.0\n",
      "tensor([0])   2   0.0\n",
      "tensor([0])   3   0.0\n",
      "tensor([0])   4   0.0\n",
      "tensor([0])   5   0.0\n",
      "tensor([1])   6   0.0\n",
      "tensor([0])   7   0.0\n",
      "tensor([0])   8   0.0\n",
      "tensor([1])   9   0.0\n",
      "tensor([0])   10   0.0\n"
     ]
    }
   ],
   "source": [
    "for index, sample_graph in enumerate(train_data):\n",
    "    label = sample_graph.label\n",
    "    networkx_sample_graph = to_networkx(sample_graph, to_undirected = True)\n",
    "    print(label, \" \", index, \" \", compute_min_clustering_coefficient(networkx_sample_graph))\n",
    "    \n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(compute_min_clustering_coefficient(from_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_min_clustering_coefficient(triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if instead of training a gnn – you just trained on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier on just the degrees of the nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier on just the triangle feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier on the triangle features and the degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a network with just degree of the node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN(\n",
      "  (conv1): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv2): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv3): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv4): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv5): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv6): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (conv7): GINConv(nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (lin1): Linear(in_features=112, out_features=112, bias=True)\n",
      "  (lin2): Linear(in_features=112, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  16577\n"
     ]
    }
   ],
   "source": [
    "model_just_degree = GIN(num_features=1)\n",
    "print(model_just_degree)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Encountered an index error. Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 0] (got interval [0, 5138])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:272\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    271\u001b[0m     index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m bestModel, highestAcc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     loss, h \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[94], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, features_to_use)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     pred, embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_to_use\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(pred)\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred\u001b[38;5;241m.\u001b[39mfloat(), batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Fall 2022/Independent Work/thesis/train-2d/gin/gin.py:43\u001b[0m, in \u001b[0;36mGIN.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Node embeddings \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h1, edge_index)\n\u001b[1;32m     45\u001b[0m     h3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(h2, edge_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/gin_conv.py:80\u001b[0m, in \u001b[0;36mGINConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     77\u001b[0m     x: OptPairTensor \u001b[38;5;241m=\u001b[39m (x, x)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:459\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[1;32m    457\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 459\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mdistribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:336\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, dim, data)\n\u001b[0;32m--> 336\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:275\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim):\n\u001b[0;32m--> 275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered an index error. Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got interval \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mIndexError\u001b[0m: Encountered an index error. Please ensure that all indices in 'edge_index' point to valid indices in the interval [0, 0] (got interval [0, 5138])"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "bestModel, highestAcc = None, 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss, h = train(train_loader, [0])\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model_just_degree, train_loader), check_accuracy(model_just_degree, test_loader)\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "#     scheduler.step(test_acc)\n",
    "    \n",
    "    if test_acc > highestAcc:\n",
    "        highestAcc = test_acc\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'number_of_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/storage.py:78\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/storage.py:103\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'number_of_nodes'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_feature_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36mgenerate_feature_vector\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_feature_vector\u001b[39m(G):\n\u001b[0;32m----> 2\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumber_of_nodes\u001b[49m(), \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m     ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m G\u001b[38;5;241m.\u001b[39mnodes():\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;66;03m# set node degree as feature\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/data.py:441\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis/lib/python3.10/site-packages/torch_geometric/data/storage.py:80\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'number_of_nodes'"
     ]
    }
   ],
   "source": [
    "generate_feature_vector(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000, 0.3000],\n",
       "        [7.0000, 0.1429],\n",
       "        [3.0000, 0.0000],\n",
       "        [5.0000, 0.0000],\n",
       "        [6.0000, 0.1333],\n",
       "        [3.0000, 0.0000],\n",
       "        [5.0000, 0.3000],\n",
       "        [6.0000, 0.1333],\n",
       "        [3.0000, 0.3333],\n",
       "        [2.0000, 0.0000],\n",
       "        [3.0000, 0.3333],\n",
       "        [5.0000, 0.2000],\n",
       "        [3.0000, 0.0000],\n",
       "        [2.0000, 0.0000],\n",
       "        [2.0000, 1.0000],\n",
       "        [3.0000, 0.6667],\n",
       "        [3.0000, 0.3333]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_graph.x[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
