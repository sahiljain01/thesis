{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing PyG Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx, to_networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_coefficient(G, node):\n",
    "    ns = [n for n in G.neighbors(node)]\n",
    "    if len(ns) <= 1:\n",
    "        return 0\n",
    "    \n",
    "    numerator = 0\n",
    "    denominator = len(ns) * (len(ns) - 1) / 2\n",
    "    for i in range(0, len(ns)):\n",
    "        for j in range(i+1, len(ns)):\n",
    "            n1, n2 = ns[i], ns[j]\n",
    "            numerator += G.has_edge(n1, n2)\n",
    "    \n",
    "    return numerator / denominator\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_vector(G):\n",
    "    x = torch.randn(G.number_of_nodes(), 4)\n",
    "    ind = 0\n",
    "    for node in G.nodes():\n",
    "        x[ind][0] = 1 # uniform\n",
    "        x[ind][1] = G.degree[node] # node degree as a scalar \n",
    "        x[ind][2] = clustering_coefficient(G, node) # triangle counting?\n",
    "        x[ind][3] = ind # node ID features\n",
    "        ind += 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        for ind, graph in enumerate(total_laman_data[0]):\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        for ind, graph in enumerate(total_laman_data[1]):\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/custom-generated.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "laman_data = LamanDataset(\"\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 64], x=[15, 4], label=[1], num_nodes=15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laman_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "proportions = [.7, .3]\n",
    "lengths = [int(p * len(laman_data)) for p in proportions]\n",
    "lengths[-1] = len(laman_data) - sum(lengths[:-1])\n",
    "\n",
    "generator1 = torch.Generator().manual_seed(42)\n",
    "train_data, test_data = random_split(laman_data, lengths, generator=generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = 256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size = 256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches:  27\n",
      "Number of test batches:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train batches: \", len(train_loader))\n",
    "print(\"Number of test batches: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 15022], x=[3840, 4], label=[256], num_nodes=3840, batch=[3840], ptr=[257])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gin.gin_k_layers import GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=4, out_features=15, bias=True)\n",
      "      (1): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=15, out_features=15, bias=True)\n",
      "      (4): ReLU()\n",
      "    ))\n",
      "  )\n",
      "  (lin1): Linear(in_features=45, out_features=45, bias=True)\n",
      "  (lin2): Linear(in_features=45, out_features=1, bias=True)\n",
      ")\n",
      "Number of parameters:  2461\n"
     ]
    }
   ],
   "source": [
    "model = GIN(layers = 1, num_features=4, dim_h=15)\n",
    "print(model)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=0.001)\n",
    "\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', min_lr=1e-6, verbose=True, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, features_to_use):\n",
    "    ind = 0\n",
    "    for batch in data:\n",
    "        optimizer.zero_grad()\n",
    "        pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "        pred = torch.squeeze(pred)\n",
    "        loss = loss_fn(pred.float(), batch.label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ind += 1\n",
    "\n",
    "    return loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, loader, features_to_use):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            y = batch.label\n",
    "            batch.label = 0\n",
    "            pred, embedding = model(batch.x[:, features_to_use], batch.edge_index, batch.batch)\n",
    "            pred = torch.squeeze(pred)\n",
    "            predictions = (pred > 0.5).long() \n",
    "            num_correct += (predictions == y).sum() \n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "    return float(num_correct)/float(num_samples)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "bestModel, highestAcc = None, 0\n",
    "\n",
    "for epoch in range(300):\n",
    "    loss, model = train(model, train_loader, [0, 1, 2, 3])\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model, train_loader, [0, 1, 2, 3]), check_accuracy(model, test_loader, [0, 1, 2, 3])\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "    \n",
    "    if test_acc > highestAcc:\n",
    "        highestAcc = test_acc\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a twenty node graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fully connected 11-node graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an 20-node graph\n",
    "n = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 30):\n",
    "    j = (np.random.randint(0, 30))\n",
    "    while j == i:\n",
    "        j = (np.random.randint(0, 30))\n",
    "        \n",
    "    print(j == i)\n",
    "    g.add_edge(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_possible_edges = set()\n",
    "for i in range(0, 30):\n",
    "    for j in range(i+1, 30):\n",
    "        if not g.has_edge(i, j):\n",
    "            all_possible_edges.add((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_possible_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "non_edges_sample_order = random.sample(all_possible_edges, 57 - 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (u,v) in non_edges_sample_order:\n",
    "    g.add_edge(u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    graph_as_data = from_networkx(g)\n",
    "    graph_as_data.x = generate_feature_vector(g)\n",
    "    validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)\n",
    "    for batch in validation_set:\n",
    "        pred = model(batch.x[:, [0]], batch.edge_index, batch.batch)\n",
    "        print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model, test_loader, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_accuracy(model, train_loader, [0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square\n",
    "import networkx as nx\n",
    "square = nx.Graph()\n",
    "square.add_edge(0, 1)\n",
    "square.add_edge(1, 3)\n",
    "square.add_edge(0, 2)\n",
    "square.add_edge(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square with cross bar (rigid)\n",
    "import networkx as nx\n",
    "square_bar = nx.Graph()\n",
    "square_bar.add_edge(0, 1)\n",
    "square_bar.add_edge(1, 3)\n",
    "square_bar.add_edge(0, 2)\n",
    "square_bar.add_edge(2, 3)\n",
    "square_bar.add_edge(0, 3)\n",
    "square_bar.add_edge(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triangle\n",
    "import networkx as nx\n",
    "triangle = nx.Graph()\n",
    "triangle.add_edge(0, 1)\n",
    "triangle.add_edge(0, 2)\n",
    "triangle.add_edge(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pentagon\n",
    "import networkx as nx\n",
    "pentagon = nx.Graph()\n",
    "pentagon.add_edge(0, 1)\n",
    "pentagon.add_edge(1, 3)\n",
    "pentagon.add_edge(3, 4)\n",
    "pentagon.add_edge(4, 2)\n",
    "pentagon.add_edge(2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_problems = [square, square_bar, triangle, pentagon]\n",
    "labels = [1, 0, 0, 1]\n",
    "\n",
    "for index, toy_problem in enumerate(toy_problems):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        graph_as_data = from_networkx(toy_problem)\n",
    "        graph_as_data.x = generate_feature_vector(toy_problem)\n",
    "#         graph_as_data.label = labels[index]\n",
    "        validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)\n",
    "        for batch in validation_set:\n",
    "            pred = model(batch.x[:, [0, 1, 2, 3]], batch.edge_index, batch.batch)\n",
    "            print(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_as_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = DataLoader([graph_as_data], batch_size = 1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in validation_set:\n",
    "    pred = bestModel(batch.x, batch.edge_index, batch.batch)\n",
    "    print(pred[0])\n",
    "    print(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LamanTestDataset(InMemoryDataset):\n",
    "    def __init__(self, root, data_dir, transform=None, pre_transform=None, pre_filter=None):\n",
    "        self.data_dir = data_dir\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_test.pt']\n",
    "        \n",
    "    def process(self):\n",
    "        # processing code here\n",
    "        total_laman_data = None\n",
    "        with gzip.open(self.data_dir, 'r') as f:\n",
    "            total_laman_data = pickle.load(f)\n",
    "            \n",
    "        data_list = []\n",
    "        ind = 0\n",
    "        # convert from graph to Data object\n",
    "        for graph in total_laman_data[0]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 1)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 0\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        ind = 0\n",
    "        for graph in total_laman_data[1]:\n",
    "#             print(ind)\n",
    "            ind += 1\n",
    "            num_nodes = nx.number_of_nodes(graph)\n",
    "#             x = torch.randn(num_nodes, 64)\n",
    "            x = generate_feature_vector(graph)\n",
    "            graph_as_data = from_networkx(graph)\n",
    "            graph_as_data.x = x\n",
    "            graph_as_data.label = 1\n",
    "            data_list.append(graph_as_data)\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add functionality to support a test dataset\n",
    "TEST_DATA_PATH = \"../data-2d/data/test-dataset-30loc-5std.pkl.gz\"\n",
    "laman_test_set = LamanTestDataset(\"\", TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "laman_test_loader = DataLoader(laman_test_set, batch_size = 2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_acc = check_accuracy(bestModel, laman_test_loader)\n",
    "print(f\"Accuracy {random_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate statistics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the clustering coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(square, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(triangle, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_coefficient(square_bar, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate graph correlating clustering coefficient to rigidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_data:\n",
    "    item = to_networkx(item)\n",
    "    print(type(item))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_geometric.utils.convert.to_networkx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work: Sahil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_laman_data = None\n",
    "with gzip.open(DATA_PATH, 'r') as f:\n",
    "    total_laman_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graph = total_laman_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sample_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_data = from_networkx(sample_graph)\n",
    "from_data = to_networkx(to_data, to_undirected = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(from_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_clustering_coefficient(G):\n",
    "    min_coefficient = 1\n",
    "    for node in G.nodes():\n",
    "        min_coefficient = min(min_coefficient, clustering_coefficient(G, node))\n",
    "        \n",
    "    return min_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sample_graph in enumerate(train_data):\n",
    "    label = sample_graph.label\n",
    "    networkx_sample_graph = to_networkx(sample_graph, to_undirected = True)\n",
    "    print(label, \" \", index, \" \", compute_min_clustering_coefficient(networkx_sample_graph))\n",
    "    \n",
    "    if index == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_min_clustering_coefficient(from_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_min_clustering_coefficient(triangle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if instead of training a gnn â€“ you just trained on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier on just the degrees of the nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier on just the triangle feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a binary classifier on the triangle features and the degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a network with just degree of the node "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_just_degree = GIN(num_features=1)\n",
    "print(model_just_degree)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "losses = []\n",
    "\n",
    "bestModel, highestAcc = None, 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss, h = train(train_loader, [0])\n",
    "    losses.append(loss)\n",
    "    print(f\"Epoch {epoch} | Train loss {loss}\")\n",
    "    train_acc, test_acc = check_accuracy(model_just_degree, train_loader), check_accuracy(model_just_degree, test_loader)\n",
    "    print(f\"Train Accuracy {train_acc} | Test Accuracy {test_acc}\")\n",
    "#     scheduler.step(test_acc)\n",
    "    \n",
    "    if test_acc > highestAcc:\n",
    "        highestAcc = test_acc\n",
    "        bestModel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_feature_vector(sample_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graph.x[:, [0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
